#!/usr/bin/env python3
"""
PaperTracer å¢å¼ºæ¼”ç¤ºè„šæœ¬
åŒ…å«ä¼šè¯æ¢å¤ã€æ™ºèƒ½é‡è¯•å’Œé«˜çº§é”™è¯¯å¤„ç†çš„å®Œæ•´æ¼”ç¤º
"""

import sys
import os
import argparse
import json
from pathlib import Path
from papertracer_config import Config, DEMO_CONFIG, PRODUCTION_CONFIG, QUICK_TEST_CONFIG
from logger import get_logger
from papertracer import GoogleScholarCrawler, print_citation_tree, save_tree_to_json

def setup_enhanced_argument_parser():
    """è®¾ç½®å¢å¼ºç‰ˆå‘½ä»¤è¡Œå‚æ•°è§£æ"""
    parser = argparse.ArgumentParser(
        description='PaperTracer å¢å¼ºæ¼”ç¤º - æ”¯æŒä¼šè¯æ¢å¤çš„Google Scholarå¼•ç”¨çˆ¬è™«',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
å¢å¼ºåŠŸèƒ½:
  - è‡ªåŠ¨ä¼šè¯çŠ¶æ€ä¿å­˜å’Œæ¢å¤
  - æ™ºèƒ½429é”™è¯¯å¤„ç†å’Œé€€é¿ç­–ç•¥
  - å¢å¼ºçš„åçˆ¬è™«æ£€æµ‹è§„é¿
  - æ›´å¥½çš„Chromeé©±åŠ¨å…¼å®¹æ€§

ç¤ºä¾‹ç”¨æ³•:
  python enhanced_demo.py --url "https://scholar.google.com/..."
  python enhanced_demo.py --resume session_20240602_123456
  python enhanced_demo.py --config production --save-session
  python enhanced_demo.py --manual-captcha --no-delays
        """
    )
    
    parser.add_argument(
        '--url', '-u',
        type=str,
        default="https://scholar.google.com/scholar?cites=11002616430871081935&as_sdt=2005&sciodt=0,5&hl=en",
        help='èµ·å§‹URL (é»˜è®¤: é¢„è®¾ç¤ºä¾‹URL)'
    )
    
    parser.add_argument(
        '--config', '-c',
        choices=['demo', 'production', 'quick'],
        default='demo',
        help='ä½¿ç”¨é¢„è®¾é…ç½® (é»˜è®¤: demo)'
    )
    
    parser.add_argument(
        '--depth', '-d',
        type=int,
        help='é€’å½’æ·±åº¦ (è¦†ç›–é…ç½®æ–‡ä»¶è®¾ç½®)'
    )
    
    parser.add_argument(
        '--max-papers', '-m',
        type=int,
        help='æ¯å±‚æœ€å¤§è®ºæ–‡æ•° (è¦†ç›–é…ç½®æ–‡ä»¶è®¾ç½®)'
    )
    
    parser.add_argument(
        '--save-session',
        action='store_true',
        help='è‡ªåŠ¨ä¿å­˜ä¼šè¯çŠ¶æ€ä»¥ä¾›æ¢å¤'
    )
    
    parser.add_argument(
        '--resume',
        type=str,
        help='ä»æŒ‡å®šä¼šè¯IDæ¢å¤ (ä¾‹å¦‚: session_20240602_123456)'
    )
    
    parser.add_argument(
        '--session-interval',
        type=int,
        default=50,
        help='æ¯éš”å¤šå°‘ä¸ªè¯·æ±‚ä¿å­˜ä¸€æ¬¡ä¼šè¯çŠ¶æ€ (é»˜è®¤: 50)'
    )
    
    parser.add_argument(
        '--aggressive-delays',
        action='store_true',
        help='ä½¿ç”¨æ›´æ¿€è¿›çš„å»¶è¿Ÿç­–ç•¥ä»¥é¿å…429é”™è¯¯'
    )
    
    parser.add_argument(
        '--output-prefix', '-p',
        type=str,
        default='enhanced_demo',
        help='è¾“å‡ºæ–‡ä»¶å‰ç¼€ (é»˜è®¤: enhanced_demo)'
    )
    
    parser.add_argument(
        '--no-html',
        action='store_true',
        help='è·³è¿‡HTMLäº¤äº’å¼å¯è§†åŒ–ç”Ÿæˆ'
    )
    
    parser.add_argument(
        '--no-visualization',
        action='store_true',
        help='è·³è¿‡æ‰€æœ‰å¯è§†åŒ–ç”Ÿæˆ'
    )
    
    parser.add_argument(
        '--no-browser',
        action='store_true',
        help='ç¦ç”¨æµè§ˆå™¨fallbackæ¨¡å¼'
    )
    
    parser.add_argument(
        '--captcha-retries',
        type=int,
        default=5,
        help='CAPTCHAé‡è¯•æ¬¡æ•° (é»˜è®¤: 5)'
    )
    
    parser.add_argument(
        '--manual-captcha',
        action='store_true',
        help='å¯ç”¨æ‰‹åŠ¨CAPTCHAå¤„ç†æ¨¡å¼ï¼ˆç¦ç”¨æ— å¤´æµè§ˆå™¨ï¼‰'
    )
    
    parser.add_argument(
        '--no-delays',
        action='store_true',
        help='ç¦ç”¨æ‰€æœ‰å»¶è¿Ÿç­–ç•¥ï¼ˆè°¨æ…ä½¿ç”¨ï¼Œå¯èƒ½å¯¼è‡´æ›´å¤š429é”™è¯¯ï¼‰'
    )
    
    parser.add_argument(
        '--skip-429',
        action='store_true',
        help='é‡åˆ°429é”™è¯¯æ—¶ç›´æ¥è·³è¿‡ï¼Œä¸è¿›è¡Œä»»ä½•ä¿®å¤å’Œé‡è¯•ï¼ˆæé€Ÿæ¨¡å¼ï¼‰'
    )
    
    parser.add_argument(
        '--verbose', '-v',
        action='store_true',
        help='è¯¦ç»†è¾“å‡º'
    )
    
    return parser

def get_enhanced_config(config_name, args):
    """è·å–å¢å¼ºé…ç½®å‚æ•°"""
    config_map = {
        'demo': DEMO_CONFIG,
        'production': PRODUCTION_CONFIG,
        'quick': QUICK_TEST_CONFIG
    }
    
    config = config_map[config_name].copy()
    
    # å‘½ä»¤è¡Œå‚æ•°è¦†ç›–é…ç½®æ–‡ä»¶
    if args.depth is not None:
        config['max_depth'] = args.depth
    if args.max_papers is not None:
        config['max_papers_per_level'] = args.max_papers
    
    # å¦‚æœä½¿ç”¨æ¿€è¿›å»¶è¿Ÿç­–ç•¥ï¼Œå¢åŠ å»¶è¿ŸèŒƒå›´
    if args.aggressive_delays:
        current_min, current_max = config['delay_range']
        config['delay_range'] = (current_min * 1.5, current_max * 2.0)
    
    # å¦‚æœç¦ç”¨å»¶è¿Ÿç­–ç•¥ï¼Œè®¾ç½®æå°çš„å»¶è¿Ÿ
    if args.no_delays:
        from logger import get_logger
        logger = get_logger()
        config['delay_range'] = (0.1, 0.3)
        logger.warning("âš ï¸  å»¶è¿Ÿç­–ç•¥å·²ç¦ç”¨ï¼Œè¿™å¯èƒ½å¯¼è‡´æ›´å¤š429é”™è¯¯ï¼")
    
    return config

def create_session_manager(session_dir, args):
    """åˆ›å»ºä¼šè¯ç®¡ç†å™¨"""
    class SessionManager:
        def __init__(self, session_dir, save_interval=50):
            self.session_dir = Path(session_dir)
            self.save_interval = save_interval
            self.session_file = self.session_dir / "session_state.json"
            self.request_counter = 0
            
        def should_save(self, crawler):
            """æ£€æŸ¥æ˜¯å¦åº”è¯¥ä¿å­˜ä¼šè¯çŠ¶æ€"""
            self.request_counter += 1
            return self.request_counter % self.save_interval == 0
            
        def save_if_needed(self, crawler):
            """å¦‚æœéœ€è¦åˆ™ä¿å­˜ä¼šè¯çŠ¶æ€"""
            if self.should_save(crawler):
                crawler.save_session_state(str(self.session_file))
                return True
            return False
            
        def force_save(self, crawler):
            """å¼ºåˆ¶ä¿å­˜ä¼šè¯çŠ¶æ€"""
            crawler.save_session_state(str(self.session_file))
            
        def load_session(self, crawler):
            """åŠ è½½ä¼šè¯çŠ¶æ€"""
            if self.session_file.exists():
                return crawler.load_session_state(str(self.session_file))
            return False
    
    return SessionManager(session_dir, args.session_interval)

def run_enhanced_demo():
    """è¿è¡Œå¢å¼ºæ¼”ç¤º"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = setup_enhanced_argument_parser()
    args = parser.parse_args()
    
    # è·å–æ—¥å¿—å™¨
    logger = get_logger()
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    if args.verbose:
        logger.logger.setLevel(10)  # DEBUG
    
    logger.info("ğŸš€ PaperTracer å¢å¼ºæ¼”ç¤ºå¼€å§‹")
    logger.info("=" * 70)
    
    try:
        # å‡†å¤‡è¾“å‡ºç›®å½•å’Œä¼šè¯ç›®å½•
        logger.info("ğŸ“ å‡†å¤‡è¾“å‡ºç›®å½•...")
        Config.ensure_output_directory()
        
        # åˆ›å»ºæœ¬æ¬¡çˆ¬å–çš„ä¼šè¯ç›®å½•
        session_dir = Config.get_timestamped_dirname(args.output_prefix)
        Config.ensure_output_directory(session_dir)
        logger.info(f"   âœ“ ä¸»è¾“å‡ºç›®å½•: {Config.OUTPUT_DIR}/")
        logger.info(f"   âœ“ ä¼šè¯ç›®å½•: {Config.OUTPUT_DIR}/{session_dir}/")
        
        # è·å–é…ç½®
        config = get_enhanced_config(args.config, args)
        logger.info(f"ğŸ”§ ä½¿ç”¨å¢å¼ºé…ç½®: {args.config}")
        logger.info(f"   - é€’å½’æ·±åº¦: {config['max_depth']}")
        logger.info(f"   - æ¯å±‚è®ºæ–‡æ•°: {config['max_papers_per_level']}")
        logger.info(f"   - å»¶è¿ŸèŒƒå›´: {config['delay_range']} ç§’")
        logger.info(f"   - CAPTCHAé‡è¯•æ¬¡æ•°: {args.captcha_retries}")
        logger.info(f"   - æµè§ˆå™¨fallback: {'ç¦ç”¨' if args.no_browser else 'å¯ç”¨'}")
        logger.info(f"   - æ¿€è¿›å»¶è¿Ÿç­–ç•¥: {'å¯ç”¨' if args.aggressive_delays else 'ç¦ç”¨'}")
        logger.info(f"   - å»¶è¿Ÿç­–ç•¥: {'ç¦ç”¨' if args.no_delays else 'å¯ç”¨'}")
        logger.info(f"   - æ‰‹åŠ¨CAPTCHAæ¨¡å¼: {'å¯ç”¨' if args.manual_captcha else 'ç¦ç”¨'}")
        logger.info(f"   - 429è·³è¿‡æ¨¡å¼: {'å¯ç”¨' if args.skip_429 else 'ç¦ç”¨'}")
        logger.info(f"   - ä¼šè¯ä¿å­˜é—´éš”: {args.session_interval} è¯·æ±‚")
        
        # åˆ›å»ºå¢å¼ºçˆ¬è™«å®ä¾‹
        logger.info("ğŸš€ åˆå§‹åŒ–å¢å¼ºçˆ¬è™«...")
        
        # è®¾ç½®æ— å¤´æµè§ˆå™¨æ¨¡å¼ï¼ˆå¦‚æœå¯ç”¨æ‰‹åŠ¨CAPTCHAï¼Œåˆ™ä½¿ç”¨æœ‰å¤´æ¨¡å¼ï¼‰
        use_headless = not args.manual_captcha
        
        crawler = GoogleScholarCrawler(
            max_depth=config['max_depth'],
            max_papers_per_level=config['max_papers_per_level'],
            delay_range=config['delay_range'],
            max_captcha_retries=args.captcha_retries,
            use_browser_fallback=not args.no_browser,
            skip_429_errors=args.skip_429
        )
        
        # å¦‚æœå¯ç”¨æ‰‹åŠ¨CAPTCHAæ¨¡å¼ï¼Œè®¾ç½®æµè§ˆå™¨ä¸ºæœ‰å¤´æ¨¡å¼
        if args.manual_captcha:
            crawler.use_headless_browser = False
            logger.info("ğŸ¯ æ‰‹åŠ¨CAPTCHAæ¨¡å¼å·²å¯ç”¨ï¼Œæµè§ˆå™¨å°†ä»¥æœ‰å¤´æ¨¡å¼è¿è¡Œ")
        
        # è®¾ç½®ä¼šè¯ç®¡ç†å™¨
        session_manager = None
        if args.save_session or args.resume:
            session_manager = create_session_manager(
                Config.get_output_path('', session_dir), 
                args
            )
            
            # å¦‚æœæŒ‡å®šäº†æ¢å¤ä¼šè¯
            if args.resume:
                # å°è¯•åŠ è½½æŒ‡å®šçš„ä¼šè¯
                resume_session_file = Config.OUTPUT_DIR / args.resume / "session_state.json"
                if resume_session_file.exists():
                    if crawler.load_session_state(str(resume_session_file)):
                        logger.info(f"âœ… æˆåŠŸæ¢å¤ä¼šè¯: {args.resume}")
                    else:
                        logger.warning(f"âš ï¸  æ¢å¤ä¼šè¯å¤±è´¥: {args.resume}")
                else:
                    logger.error(f"âŒ ä¼šè¯æ–‡ä»¶ä¸å­˜åœ¨: {resume_session_file}")
                    return False
            
            logger.info("ğŸ“Š ä¼šè¯ç®¡ç†å·²å¯ç”¨")
        
        # æ˜¾ç¤ºèµ·å§‹URL
        logger.info(f"ğŸ“‹ èµ·å§‹URL: {args.url}")
        
        # çˆ¬å–å¼•ç”¨æ ‘
        logger.info("ğŸ” å¼€å§‹çˆ¬å–å¼•ç”¨æ ‘...")
        logger.info("   (æ ¹æ®é…ç½®ï¼Œè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿåˆ°å‡ ååˆ†é’Ÿ...)")
        
        # åŒ…è£…build_citation_treeæ–¹æ³•ä»¥æ”¯æŒä¼šè¯ä¿å­˜
        original_build_method = crawler.build_citation_tree
        
        def enhanced_build_citation_tree(url, current_depth=0):
            """å¢å¼ºç‰ˆæ„å»ºå¼•ç”¨æ ‘ï¼Œæ”¯æŒä¼šè¯ä¿å­˜"""
            result = original_build_method(url, current_depth)
            
            # åœ¨ä¼šè¯ç®¡ç†å™¨ä¸­ä¿å­˜çŠ¶æ€
            if session_manager and args.save_session:
                if session_manager.save_if_needed(crawler):
                    logger.info("ğŸ’¾ è‡ªåŠ¨ä¿å­˜ä¼šè¯çŠ¶æ€")
            
            return result
        
        # æ›¿æ¢æ–¹æ³•
        crawler.build_citation_tree = enhanced_build_citation_tree
        
        citation_tree = crawler.build_citation_tree(args.url)
        
        # æœ€ç»ˆä¿å­˜ä¼šè¯çŠ¶æ€
        if session_manager and args.save_session:
            session_manager.force_save(crawler)
            logger.info("ğŸ’¾ æœ€ç»ˆä¼šè¯çŠ¶æ€å·²ä¿å­˜")
        
        if not citation_tree:
            logger.error("âŒ æ— æ³•æ„å»ºå¼•ç”¨æ ‘")
            return False
        
        logger.info("âœ… å¼•ç”¨æ ‘æ„å»ºæˆåŠŸ!")
        logger.info(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
        logger.info(f"   - æ€»è¯·æ±‚æ•°: {crawler.request_count}")
        logger.info(f"   - å·²è®¿é—®URLæ•°: {len(crawler.visited_urls)}")
        logger.info(f"   - è¿ç»­429é”™è¯¯æ¬¡æ•°: {crawler.consecutive_429_count}")
        
        # æ˜¾ç¤ºç»“æœ
        logger.info("ğŸ“Š æ˜¾ç¤ºçˆ¬å–ç»“æœ...")
        print("-" * 70)
        print_citation_tree(citation_tree)
        
        # ä¿å­˜æ•°æ®
        logger.info("ğŸ’¾ ä¿å­˜æ•°æ®...")
        json_filename = Config.get_timestamped_filename(
            prefix="enhanced_citation_tree",
            suffix="",
            extension="json"
        )
        json_path = Config.get_output_path(json_filename, session_dir)
        save_tree_to_json(citation_tree, json_path)
        logger.info(f"   âœ“ æ•°æ®å·²ä¿å­˜åˆ°: {json_path}")
        
        # åˆ›å»ºå¯è§†åŒ–
        if not args.no_visualization:
            logger.info("ğŸ¨ åˆ›å»ºå¯è§†åŒ–å›¾è¡¨...")
            try:
                from visualize_tree import CitationTreeVisualizer
                
                visualizer = CitationTreeVisualizer(json_path)
                
                # ç®€å•ç½‘ç»œå›¾
                logger.info("   æ­£åœ¨åˆ›å»ºç½‘ç»œå›¾...")
                simple_filename = Config.get_timestamped_filename(
                    prefix="enhanced_simple",
                    suffix="",
                    extension="png"
                )
                simple_path = Config.get_output_path(simple_filename, session_dir)
                visualizer.create_simple_visualization(
                    simple_path, 
                    figsize=config['figsize']
                )
                
                # ç»Ÿè®¡å›¾è¡¨
                logger.info("   æ­£åœ¨åˆ›å»ºç»Ÿè®¡å›¾è¡¨...")
                stats_filename = Config.get_timestamped_filename(
                    prefix="enhanced_stats",
                    suffix="",
                    extension="png"
                )
                stats_path = Config.get_output_path(stats_filename, session_dir)
                visualizer.create_statistics_plot(stats_path)
                
                # åˆ›å»ºäº¤äº’å¼HTMLå¯è§†åŒ–
                if not args.no_html:
                    logger.info("   æ­£åœ¨åˆ›å»ºäº¤äº’å¼HTMLå¯è§†åŒ–...")
                    try:
                        from html_visualizer import InteractiveHTMLVisualizer
                        
                        html_visualizer = InteractiveHTMLVisualizer(json_path)
                        html_filename = Config.get_timestamped_filename(
                            prefix="enhanced_interactive",
                            suffix="",
                            extension="html"
                        )
                        html_path = Config.get_output_path(html_filename, session_dir)
                        html_visualizer.create_interactive_html(html_path)
                        
                        logger.info("âœ… å¢å¼ºå¯è§†åŒ–å›¾è¡¨åˆ›å»ºå®Œæˆ!")
                        logger.info(f"ğŸ“ è¾“å‡ºæ–‡ä»¶å¤¹: {Config.OUTPUT_DIR}/{session_dir}/")
                        logger.info(f"ğŸ“„ è¾“å‡ºæ–‡ä»¶:")
                        logger.info(f"   - {os.path.basename(json_path)} (æ•°æ®)")
                        logger.info(f"   - {os.path.basename(simple_path)} (ç½‘ç»œå›¾)")
                        logger.info(f"   - {os.path.basename(stats_path)} (ç»Ÿè®¡å›¾)")
                        logger.info(f"   - {os.path.basename(html_path)} (äº¤äº’å¼ç½‘é¡µ)")
                        if session_manager and args.save_session:
                            logger.info(f"   - session_state.json (ä¼šè¯çŠ¶æ€)")
                        logger.info(f"ğŸŒ åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€ {html_path} æŸ¥çœ‹äº¤äº’å¼å¯è§†åŒ–")
                        
                    except Exception as html_e:
                        logger.warning(f"âš ï¸  HTMLå¯è§†åŒ–åˆ›å»ºå¤±è´¥: {html_e}")
                        logger.info("âœ… é™æ€å¯è§†åŒ–å›¾è¡¨åˆ›å»ºå®Œæˆ!")
                        
            except ImportError as e:
                logger.warning(f"âš ï¸  å¯è§†åŒ–åŠŸèƒ½éœ€è¦é¢å¤–ä¾èµ–: {e}")
                logger.info("ğŸ’¡ å¯ä»¥è¿è¡Œ: pip install matplotlib networkx")
            except Exception as e:
                logger.error(f"âš ï¸  å¯è§†åŒ–è¿‡ç¨‹ä¸­å‡ºç°é—®é¢˜: {e}")
        else:
            logger.info("â­ï¸  è·³è¿‡æ‰€æœ‰å¯è§†åŒ– (ä½¿ç”¨äº† --no-visualization)")
        
        # æ˜¾ç¤ºå®Œæˆä¿¡æ¯
        logger.info("ğŸ‰ å¢å¼ºæ¼”ç¤ºå®Œæˆ!")
        logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {Config.OUTPUT_DIR}/{session_dir}/")
        
        if session_manager and args.save_session:
            logger.info("ğŸ’¡ ä¼šè¯æ¢å¤æç¤º:")
            logger.info(f"   è¦æ¢å¤æ­¤ä¼šè¯ï¼Œè¯·è¿è¡Œ:")
            logger.info(f"   python enhanced_demo.py --resume {session_dir}")
        
        logger.info("ğŸ“Š å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æŸ¥çœ‹è¾“å‡ºæ–‡ä»¶:")
        logger.info(f"   ls {Config.OUTPUT_DIR}/{session_dir}/")
        
        return True
        
    except KeyboardInterrupt:
        logger.warning("âš ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        # å¦‚æœå¯ç”¨äº†ä¼šè¯ä¿å­˜ï¼Œå°è¯•ä¿å­˜å½“å‰çŠ¶æ€
        if 'session_manager' in locals() and 'crawler' in locals() and args.save_session:
            try:
                session_manager.force_save(crawler)
                logger.info("ğŸ’¾ ä¸­æ–­å‰å·²ä¿å­˜ä¼šè¯çŠ¶æ€")
            except:
                pass
        return False
    except Exception as e:
        logger.error(f"âŒ å¢å¼ºæ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        if args.verbose:
            import traceback
            logger.error(traceback.format_exc())
        return False

if __name__ == "__main__":
    success = run_enhanced_demo()
    sys.exit(0 if success else 1)

'''
python /Users/shufanzhang/Documents/coderepos/papertracer/enhanced_demo.py --url "https://scholar.google.com/scholar?cites=10749086880817846297&as_sdt=2005&sciodt=0,5&hl=en" --config demo --depth 5 --max-papers 10 --manual-captcha --no-delays

# ä½¿ç”¨429è·³è¿‡æ¨¡å¼çš„ç¤ºä¾‹ï¼ˆæé€Ÿæ¨¡å¼ï¼Œè·³è¿‡æ‰€æœ‰429é”™è¯¯ï¼‰:
python /Users/shufanzhang/Documents/coderepos/papertracer/enhanced_demo.py --url "https://scholar.google.com/scholar?cites=10749086880817846297&as_sdt=2005&sciodt=0,5&hl=en" --config demo --depth 3 --max-papers 5 --skip-429 --no-delays
'''