"""
Google Scholar å¼•ç”¨çˆ¬è™«
é€’å½’çˆ¬å–æ–‡ç« çš„å¼•ç”¨å…³ç³»å¹¶è¿”å›æ ‘å½¢æ•°æ®ç»“æ„
"""

import requests
from bs4 import BeautifulSoup
import time
import random
import re
from urllib.parse import urljoin, parse_qs, urlparse
from dataclasses import dataclass
from typing import List, Dict, Optional, Set
import json
import logging
import platform
from pathlib import Path
from tempfile import NamedTemporaryFile
import traceback

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# å°è¯•å¯¼å…¥æ— å¤´æµè§ˆå™¨ç›¸å…³ä¾èµ–ï¼ˆå¯é€‰çš„ï¼Œç”¨äºå¤„ç†CAPTCHAï¼‰
BROWSER_AVAILABLE = False
try:
    import undetected_chromedriver as uc
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.common.exceptions import TimeoutException, WebDriverException
    BROWSER_AVAILABLE = True
    logger.info("æµè§ˆå™¨æ¨¡å—å·²åŠ è½½ï¼Œå¯ç”¨äºå¤„ç†CAPTCHA")
except ImportError:
    logger.warning("æµè§ˆå™¨æ¨¡å—å¯¼å…¥å¤±è´¥ï¼Œæ— æ³•ä½¿ç”¨æµè§ˆå™¨ç»•è¿‡CAPTCHAã€‚è€ƒè™‘å®‰è£…: pip install undetected-chromedriver selenium")

@dataclass
class Paper:
    """è®ºæ–‡æ•°æ®ç»“æ„"""
    title: str
    authors: str = ""
    year: str = ""
    citation_count: int = 0
    url: str = ""
    cited_by_url: str = ""
    abstract: str = ""

@dataclass 
class CitationNode:
    """å¼•ç”¨æ ‘èŠ‚ç‚¹"""
    paper: Paper
    children: List['CitationNode']
    depth: int = 0

class GoogleScholarCrawler:
    """Google Scholar çˆ¬è™«ç±»"""
    
    def __init__(self, max_depth=3, max_papers_per_level=10, delay_range=(2, 5), max_captcha_retries=3,
                 use_browser_fallback=True, captcha_service_api_key=None, proxy_list=None):
        self.max_depth = max_depth
        self.max_papers_per_level = max_papers_per_level
        self.delay_range = delay_range
        self.max_captcha_retries = max_captcha_retries
        self.use_browser_fallback = use_browser_fallback and BROWSER_AVAILABLE
        self.captcha_service_api_key = captcha_service_api_key
        self.proxy_list = proxy_list or []
        self.proxy_index = 0
        self.visited_urls: Set[str] = set()
        self.session = requests.Session()
        self.user_agents = [
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0'
        ]
        self.request_count = 0
        self.browser = None
        
        # è®¾ç½®æ›´å®Œæ•´çš„è¯·æ±‚å¤´
        self._update_headers()
    
    def _get_random_delay(self):
        """è·å–éšæœºå»¶è¿Ÿæ—¶é—´ï¼ˆå·²åºŸå¼ƒï¼Œä½¿ç”¨_adaptive_delayä»£æ›¿ï¼‰"""
        return random.uniform(*self.delay_range)
    
    def _extract_cluster_id(self, url: str) -> Optional[str]:
        """ä»URLä¸­æå–cluster ID"""
        try:
            parsed = urlparse(url)
            params = parse_qs(parsed.query)
            if 'cites' in params:
                return params['cites'][0]
            elif 'cluster' in params:
                return params['cluster'][0]
        except:
            pass
        return None
    
    def _parse_paper_info(self, result_div) -> Paper:
        """è§£æå•ç¯‡è®ºæ–‡ä¿¡æ¯"""
        try:
            # æå–æ ‡é¢˜ - å°è¯•å¤šç§é€‰æ‹©å™¨
            title_elem = (result_div.find('h3', class_='gs_rt') or 
                         result_div.find('h3') or 
                         result_div.find('a', class_='gs_rt'))
            
            if title_elem:
                # å¦‚æœæ ‡é¢˜åœ¨é“¾æ¥å†…ï¼Œæå–é“¾æ¥æ–‡æœ¬
                title_link = title_elem.find('a') if title_elem.name != 'a' else title_elem
                if title_link:
                    title = title_link.get_text(strip=True)
                else:
                    title = title_elem.get_text(strip=True)
            else:
                title = "Unknown Title"
            
            # è¿‡æ»¤æ‰æ˜æ˜¾çš„é”™è¯¯æ ‡é¢˜
            if not title or title.lower() in ['unknown title', 'parse error', '']:
                logger.debug("å‘ç°ç©ºæ ‡é¢˜æˆ–é”™è¯¯æ ‡é¢˜ï¼Œè·³è¿‡")
                return Paper(title="Parse Error", authors="", year="")
            
            # æå–ä½œè€…å’Œå¹´ä»½
            authors_elem = result_div.find('div', class_='gs_a')
            authors_text = authors_elem.get_text(strip=True) if authors_elem else ""
            
            # å°è¯•æå–å¹´ä»½
            year_match = re.search(r'\b(19|20)\d{2}\b', authors_text)
            year = year_match.group() if year_match else ""
            
            # æå–ä½œè€…ï¼ˆå¹´ä»½å‰çš„éƒ¨åˆ†ï¼‰
            if year:
                authors = authors_text.split(year)[0].strip(' -,')
            else:
                authors = authors_text
            
            # æ¸…ç†ä½œè€…ä¿¡æ¯
            authors = re.sub(r'\s*-\s*$', '', authors)  # ç§»é™¤æœ«å°¾çš„ç ´æŠ˜å·
            authors = re.sub(r'\s+', ' ', authors)  # æ ‡å‡†åŒ–ç©ºæ ¼
            
            # æå–å¼•ç”¨æ¬¡æ•°
            cite_elem = result_div.find('a', string=re.compile(r'Cited by \d+', re.IGNORECASE))
            if not cite_elem:
                # å°è¯•å…¶ä»–å¯èƒ½çš„å¼•ç”¨é“¾æ¥æ ¼å¼
                cite_elem = result_div.find('a', href=re.compile(r'cites='))
            
            citation_count = 0
            cited_by_url = ""
            
            if cite_elem:
                cite_text = cite_elem.get_text(strip=True)
                cite_match = re.search(r'Cited by (\d+)', cite_text, re.IGNORECASE)
                if cite_match:
                    citation_count = int(cite_match.group(1))
                    cited_by_url = urljoin('https://scholar.google.com', cite_elem.get('href', ''))
            
            # æå–è®ºæ–‡URL
            paper_url = ""
            if title_elem:
                link_elem = title_elem.find('a') if title_elem.name != 'a' else title_elem
                if link_elem:
                    paper_url = link_elem.get('href', '')
            
            # æå–æ‘˜è¦
            abstract_elem = result_div.find('span', class_='gs_rs')
            abstract = abstract_elem.get_text(strip=True) if abstract_elem else ""
            
            paper = Paper(
                title=title,
                authors=authors,
                year=year,
                citation_count=citation_count,
                url=paper_url,
                cited_by_url=cited_by_url,
                abstract=abstract
            )
            
            logger.debug(f"è§£æè®ºæ–‡æˆåŠŸ: {title[:50]}...")
            return paper
        
        except Exception as e:
            logger.error(f"è§£æè®ºæ–‡ä¿¡æ¯æ—¶å‡ºé”™: {e}")
            return Paper(title="Parse Error", authors="", year="")
    
    def _fetch_citations(self, cited_by_url: str) -> List[Paper]:
        """è·å–å¼•ç”¨è¯¥è®ºæ–‡çš„æ–‡ç« åˆ—è¡¨"""
        if not cited_by_url or cited_by_url in self.visited_urls:
            return []
        
        self.visited_urls.add(cited_by_url)  # Mark as visited once we start processing it
        
        attempt = 0
        browser_attempt = False  # æ ‡è®°æ˜¯å¦å·²å°è¯•ä½¿ç”¨æµè§ˆå™¨
        
        while attempt < self.max_captcha_retries:
            try:
                logger.info(f"æ­£åœ¨çˆ¬å–: {cited_by_url} (å°è¯• {attempt + 1}/{self.max_captcha_retries})")
                
                # ä½¿ç”¨æµè§ˆå™¨fallbackå°è¯•ç»•è¿‡CAPTCHA
                if browser_attempt and self.use_browser_fallback:
                    logger.info("å°è¯•ä½¿ç”¨æµè§ˆå™¨æ–¹å¼ç»•è¿‡CAPTCHA...")
                    html_content = self._fetch_with_browser(cited_by_url)
                    if html_content:
                        soup = BeautifulSoup(html_content, 'html.parser')
                    else:
                        logger.warning("æµè§ˆå™¨è·å–å†…å®¹å¤±è´¥ï¼Œè¿”å›ç©ºç»“æœ")
                        return []
                else:
                    # å¸¸è§„è¯·æ±‚æ–¹æ³•
                    self._adaptive_delay()
                    
                    if self.request_count % 5 == 0:
                        self._update_headers()
                    
                    response = self.session.get(cited_by_url, timeout=20)
                    response.raise_for_status()
                    
                    soup = BeautifulSoup(response.content, 'html.parser')
                
                # æ£€æµ‹CAPTCHAæˆ–å°ç¦
                if self._is_captcha_page(soup):
                    logger.warning(f"CAPTCHA æ£€æµ‹äº: {cited_by_url} (å°è¯• {attempt + 1})")
                    
                    # å¦‚æœè¿˜æ²¡å°è¯•è¿‡æµè§ˆå™¨æ–¹æ³•ï¼Œä¸”é…ç½®å…è®¸ï¼Œåˆ™å°è¯•æµè§ˆå™¨æ–¹æ³•
                    if not browser_attempt and self.use_browser_fallback:
                        logger.info("æ£€æµ‹åˆ°CAPTCHAï¼Œåˆ‡æ¢åˆ°æµè§ˆå™¨æ–¹å¼å°è¯•...")
                        browser_attempt = True
                        # ä¸å¢åŠ å°è¯•æ¬¡æ•°ï¼Œç›´æ¥è¿›å…¥ä¸‹ä¸€å¾ªç¯ç”¨æµè§ˆå™¨è®¿é—®
                        continue
                    
                    # å·²ç»å°è¯•è¿‡æµè§ˆå™¨æˆ–ä¸å…è®¸ä½¿ç”¨æµè§ˆå™¨ï¼Œåˆ™å¤„ç†CAPTCHA
                    self._handle_captcha_or_block(cited_by_url, response.text if 'response' in locals() else "", attempt)
                    attempt += 1
                    
                    if attempt >= self.max_captcha_retries:
                        logger.error(f"CAPTCHA éªŒè¯å¤±è´¥æ¬¡æ•°è¿‡å¤šï¼Œæ”¾å¼ƒçˆ¬å–: {cited_by_url}")
                        return []
                    
                    logger.info(f"CAPTCHA åç­‰å¾…åé‡è¯• ({attempt + 1}/{self.max_captcha_retries}): {cited_by_url}")
                    browser_attempt = False  # é‡ç½®æµè§ˆå™¨å°è¯•çŠ¶æ€ï¼Œå†æ¬¡ä»å¸¸è§„è¯·æ±‚å¼€å§‹
                    continue
                
                # æŸ¥æ‰¾æ‰€æœ‰è®ºæ–‡ç»“æœ
                paper_divs = soup.find_all('div', class_='gs_r')
                
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç»“æœï¼Œå¯èƒ½æ˜¯é¡µé¢ç»“æ„å‘ç”Ÿäº†å˜åŒ–
                if not paper_divs:
                    logger.warning(f"æœªæ‰¾åˆ°è®ºæ–‡ç»“æœï¼Œå¯èƒ½é¡µé¢ç»“æ„å·²å˜åŒ–: {cited_by_url}")
                    # å°è¯•å…¶ä»–å¯èƒ½çš„é€‰æ‹©å™¨
                    paper_divs = soup.find_all('div', class_='gs_ri') or soup.find_all('div', {'data-lid': True})
                
                papers = []
                
                for div in paper_divs[:self.max_papers_per_level]:
                    paper = self._parse_paper_info(div)
                    if paper.title != "Parse Error" and paper.title != "Unknown Title":
                        papers.append(paper)
                
                logger.info(f"æ‰¾åˆ° {len(papers)} ç¯‡æœ‰æ•ˆå¼•ç”¨è®ºæ–‡ from {cited_by_url}")
                
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•æœ‰æ•ˆè®ºæ–‡ï¼Œè®°å½•è°ƒè¯•ä¿¡æ¯
                if not papers and paper_divs:
                    debug_file = f"debug_no_papers_{int(time.time())}.html"
                    with open(debug_file, 'w', encoding='utf-8') as f:
                        f.write(soup.prettify())
                    logger.warning(f"è§£ææˆåŠŸä½†æœªæå–åˆ°è®ºæ–‡ï¼Œå·²ä¿å­˜è°ƒè¯•é¡µé¢åˆ°: {debug_file}")
                
                return papers  # Success
            
            except requests.exceptions.RequestException as e:
                logger.error(f"ç½‘ç»œè¯·æ±‚å¤±è´¥ ({cited_by_url}): {e} (å°è¯• {attempt + 1}/{self.max_captcha_retries})")
                attempt += 1
                
                # å¦‚æœæ˜¯ç½‘ç»œé”™è¯¯ä¸”æœªå°è¯•è¿‡æµè§ˆå™¨ï¼Œåˆ‡æ¢åˆ°æµè§ˆå™¨æ¨¡å¼å°è¯•
                if not browser_attempt and self.use_browser_fallback:
                    logger.info("ç½‘ç»œè¯·æ±‚å¤±è´¥ï¼Œåˆ‡æ¢åˆ°æµè§ˆå™¨æ–¹å¼å°è¯•...")
                    browser_attempt = True
                    continue
                
                if attempt >= self.max_captcha_retries:
                    logger.error(f"ç½‘ç»œè¯·æ±‚å¤±è´¥æ¬¡æ•°è¿‡å¤šï¼Œæ”¾å¼ƒçˆ¬å–: {cited_by_url}")
                    return []
                    
                time.sleep(random.uniform(3, 7) * (attempt + 1))
                browser_attempt = False  # é‡ç½®æµè§ˆå™¨å°è¯•çŠ¶æ€
                continue
                
            except Exception as e:
                logger.error(f"è§£æé¡µé¢æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯ ({cited_by_url}): {e} (å°è¯• {attempt + 1}/{self.max_captcha_retries})")
                # ä¿å­˜è°ƒè¯• HTML for unexpected errors during parsing
                if 'response' in locals() and hasattr(response, 'text'):
                    debug_file = f"debug_parse_error_{int(time.time())}.html"
                    try:
                        with open(debug_file, 'w', encoding='utf-8') as f:
                            f.write(response.text)
                        logger.warning(f"æœªçŸ¥è§£æé”™è¯¯ï¼Œå·²ä¿å­˜è°ƒè¯•é¡µé¢åˆ°: {debug_file}")
                    except Exception as e_debug:
                        logger.error(f"ä¿å­˜è°ƒè¯•é¡µé¢å¤±è´¥: {e_debug}")
                
                attempt += 1
                if attempt >= self.max_captcha_retries:
                    logger.error(f"æœªçŸ¥é”™è¯¯æ¬¡æ•°è¿‡å¤šï¼Œæ”¾å¼ƒçˆ¬å–: {cited_by_url}")
                    return []
                time.sleep(random.uniform(3, 7) * (attempt + 1))
                browser_attempt = False  # é‡ç½®æµè§ˆå™¨å°è¯•çŠ¶æ€
                continue
        
        logger.error(f"æ‰€æœ‰å°è¯•å‡å¤±è´¥: {cited_by_url}")
        return []

    def _get_paper_from_scholar_url(self, scholar_url: str) -> Optional[Paper]:
        """ä»Scholaræœç´¢URLè·å–åŸå§‹è®ºæ–‡ä¿¡æ¯"""
        attempt = 0
        browser_attempt = False  # æ ‡è®°æ˜¯å¦å·²å°è¯•ä½¿ç”¨æµè§ˆå™¨
        
        while attempt < self.max_captcha_retries:
            try:
                logger.info(f"è·å–åŸå§‹è®ºæ–‡ä¿¡æ¯: {scholar_url} (å°è¯• {attempt + 1}/{self.max_captcha_retries})")
                
                # ä½¿ç”¨æµè§ˆå™¨fallbackå°è¯•ç»•è¿‡CAPTCHA
                if browser_attempt and self.use_browser_fallback:
                    logger.info("å°è¯•ä½¿ç”¨æµè§ˆå™¨æ–¹å¼ç»•è¿‡CAPTCHA...")
                    html_content = self._fetch_with_browser(scholar_url)
                    if html_content:
                        soup = BeautifulSoup(html_content, 'html.parser')
                    else:
                        logger.warning("æµè§ˆå™¨è·å–å†…å®¹å¤±è´¥ï¼Œè¿”å›None")
                        return None
                else:
                    # å¸¸è§„è¯·æ±‚æ–¹æ³•
                    self._adaptive_delay()
                    
                    if self.request_count % 5 == 0:
                        self._update_headers()
                    
                    response = self.session.get(scholar_url, timeout=20)
                    response.raise_for_status()
                    
                    soup = BeautifulSoup(response.content, 'html.parser')
                
                # æ£€æµ‹CAPTCHAæˆ–å°ç¦
                if self._is_captcha_page(soup):
                    logger.warning(f"CAPTCHA æ£€æµ‹äºè·å–åŸå§‹è®ºæ–‡: {scholar_url} (å°è¯• {attempt + 1})")
                    
                    # å¦‚æœè¿˜æ²¡å°è¯•è¿‡æµè§ˆå™¨æ–¹æ³•ï¼Œä¸”é…ç½®å…è®¸ï¼Œåˆ™å°è¯•æµè§ˆå™¨æ–¹æ³•
                    if not browser_attempt and self.use_browser_fallback:
                        logger.info("æ£€æµ‹åˆ°CAPTCHAï¼Œåˆ‡æ¢åˆ°æµè§ˆå™¨æ–¹å¼å°è¯•...")
                        browser_attempt = True
                        # ä¸å¢åŠ å°è¯•æ¬¡æ•°ï¼Œç›´æ¥è¿›å…¥ä¸‹ä¸€å¾ªç¯ç”¨æµè§ˆå™¨è®¿é—®
                        continue
                    
                    # å·²ç»å°è¯•è¿‡æµè§ˆå™¨æˆ–ä¸å…è®¸ä½¿ç”¨æµè§ˆå™¨ï¼Œåˆ™å¤„ç†CAPTCHA
                    self._handle_captcha_or_block(scholar_url, response.text if 'response' in locals() else "", attempt)
                    attempt += 1
                    
                    if attempt >= self.max_captcha_retries:
                        logger.error(f"CAPTCHA éªŒè¯å¤±è´¥æ¬¡æ•°è¿‡å¤šï¼Œæ”¾å¼ƒè·å–åŸå§‹è®ºæ–‡: {scholar_url}")
                        return None
                    
                    logger.info(f"CAPTCHA åç­‰å¾…åé‡è¯• ({attempt + 1}/{self.max_captcha_retries}): {scholar_url}")
                    browser_attempt = False  # é‡ç½®æµè§ˆå™¨å°è¯•çŠ¶æ€ï¼Œå†æ¬¡ä»å¸¸è§„è¯·æ±‚å¼€å§‹
                    continue
                
                # æŸ¥æ‰¾ç¬¬ä¸€ä¸ªæœç´¢ç»“æœ
                first_result = soup.find('div', class_='gs_r')
                if not first_result:
                    # å°è¯•å…¶ä»–å¯èƒ½çš„é€‰æ‹©å™¨
                    first_result = soup.find('div', class_='gs_ri') or soup.find('div', {'data-lid': True})
                
                if first_result:
                    return self._parse_paper_info(first_result)
                else:
                    logger.warning(f"æœªæ‰¾åˆ°æœç´¢ç»“æœ for {scholar_url}")
                    # å¦‚æœæ˜¯æµè§ˆå™¨æ–¹å¼è·å–çš„ç»“æœï¼Œä¿å­˜é¡µé¢è¿›è¡Œè°ƒè¯•
                    if browser_attempt:
                        debug_file = f"debug_browser_no_results_{int(time.time())}.html"
                        with open(debug_file, 'w', encoding='utf-8') as f:
                            f.write(soup.prettify())
                        logger.warning(f"æµè§ˆå™¨è·å–é¡µé¢æ— ç»“æœï¼Œå·²ä¿å­˜è°ƒè¯•é¡µé¢åˆ°: {debug_file}")
                    
                    # å¦‚æœé¡µé¢åŠ è½½æˆåŠŸä½†æ²¡æœ‰ç»“æœï¼Œå¯èƒ½æ˜¯çœŸçš„æ‰¾ä¸åˆ°
                    return None
            
            except requests.exceptions.RequestException as e:
                logger.error(f"ç½‘ç»œè¯·æ±‚å¤±è´¥è·å–åŸå§‹è®ºæ–‡ ({scholar_url}): {e} (å°è¯• {attempt + 1}/{self.max_captcha_retries})")
                attempt += 1
                
                # å¦‚æœæ˜¯ç½‘ç»œé”™è¯¯ä¸”æœªå°è¯•è¿‡æµè§ˆå™¨ï¼Œåˆ‡æ¢åˆ°æµè§ˆå™¨æ¨¡å¼å°è¯•
                if not browser_attempt and self.use_browser_fallback:
                    logger.info("ç½‘ç»œè¯·æ±‚å¤±è´¥ï¼Œåˆ‡æ¢åˆ°æµè§ˆå™¨æ–¹å¼å°è¯•...")
                    browser_attempt = True
                    continue
                
                if attempt >= self.max_captcha_retries:
                    logger.error(f"ç½‘ç»œè¯·æ±‚å¤±è´¥æ¬¡æ•°è¿‡å¤šï¼Œæ”¾å¼ƒè·å–åŸå§‹è®ºæ–‡: {scholar_url}")
                    return None
                
                time.sleep(random.uniform(3, 7) * (attempt + 1))
                browser_attempt = False  # é‡ç½®æµè§ˆå™¨å°è¯•çŠ¶æ€
                continue
                
            except Exception as e:
                logger.error(f"è·å–åŸå§‹è®ºæ–‡ä¿¡æ¯æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯ ({scholar_url}): {e} (å°è¯• {attempt + 1}/{self.max_captcha_retries})")
                if 'response' in locals() and hasattr(response, 'text'):
                    debug_file = f"debug_get_paper_error_{int(time.time())}.html"
                    try:
                        with open(debug_file, 'w', encoding='utf-8') as f:
                            f.write(response.text)
                        logger.warning(f"æœªçŸ¥é”™è¯¯è·å–åŸå§‹è®ºæ–‡ï¼Œå·²ä¿å­˜è°ƒè¯•é¡µé¢åˆ°: {debug_file}")
                    except Exception as e_debug:
                        logger.error(f"ä¿å­˜è°ƒè¯•é¡µé¢å¤±è´¥: {e_debug}")
                
                attempt += 1
                if attempt >= self.max_captcha_retries:
                    logger.error(f"æœªçŸ¥é”™è¯¯æ¬¡æ•°è¿‡å¤šï¼Œæ”¾å¼ƒè·å–åŸå§‹è®ºæ–‡: {scholar_url}")
                    return None
                
                time.sleep(random.uniform(3, 7) * (attempt + 1))
                browser_attempt = False  # é‡ç½®æµè§ˆå™¨å°è¯•çŠ¶æ€
                continue
        
        logger.error(f"æ‰€æœ‰å°è¯•å‡å¤±è´¥è·å–åŸå§‹è®ºæ–‡: {scholar_url}")
        return None

    def build_citation_tree(self, start_url: str, current_depth: int = 0) -> Optional[CitationNode]:
        """é€’å½’æ„å»ºå¼•ç”¨æ ‘"""
        if current_depth >= self.max_depth:
            return None
        
        logger.info(f"æ„å»ºå¼•ç”¨æ ‘ï¼Œæ·±åº¦: {current_depth}")
        
        # å¦‚æœæ˜¯èµ·å§‹URLï¼Œéœ€è¦å…ˆè·å–åŸå§‹è®ºæ–‡ä¿¡æ¯
        if current_depth == 0:
            if 'cites=' in start_url:
                # è¿™æ˜¯ä¸€ä¸ª"è¢«å¼•ç”¨"é¡µé¢ï¼Œéœ€è¦è·å–åŸå§‹è®ºæ–‡ä¿¡æ¯
                # æ„é€ ä¸€ä¸ªè™šæ‹Ÿçš„æ ¹è®ºæ–‡
                root_paper = Paper(
                    title="Root Paper (ä»å¼•ç”¨é¡µé¢å¼€å§‹)",
                    authors="Unknown",
                    year="",
                    citation_count=0,
                    url="",
                    cited_by_url=start_url
                )
            else:
                # å°è¯•ä»æœç´¢ç»“æœè·å–è®ºæ–‡ä¿¡æ¯
                root_paper = self._get_paper_from_scholar_url(start_url)
                if not root_paper:
                    logger.error("æ— æ³•è·å–èµ·å§‹è®ºæ–‡ä¿¡æ¯")
                    return None
        else:
            # è¿™ç§æƒ…å†µåœ¨é€’å½’è°ƒç”¨ä¸­ä¸åº”è¯¥å‘ç”Ÿ
            return None
        
        # åˆ›å»ºæ ¹èŠ‚ç‚¹
        root_node = CitationNode(paper=root_paper, children=[], depth=current_depth)
        
        # è·å–å¼•ç”¨è¿™ç¯‡è®ºæ–‡çš„æ–‡ç« 
        citing_papers = self._fetch_citations(root_paper.cited_by_url)
        
        # ä¸ºæ¯ä¸ªå¼•ç”¨è®ºæ–‡é€’å½’æ„å»ºå­æ ‘
        for citing_paper in citing_papers:
            if citing_paper.cited_by_url and current_depth + 1 < self.max_depth:
                child_node = self._build_citation_subtree(citing_paper, current_depth + 1)
                if child_node:
                    root_node.children.append(child_node)
            else:
                # å¶å­èŠ‚ç‚¹
                leaf_node = CitationNode(paper=citing_paper, children=[], depth=current_depth + 1)
                root_node.children.append(leaf_node)
        
        return root_node
    
    def _build_citation_subtree(self, paper: Paper, depth: int) -> Optional[CitationNode]:
        """é€’å½’æ„å»ºå¼•ç”¨å­æ ‘"""
        if depth >= self.max_depth:
            return None
        
        node = CitationNode(paper=paper, children=[], depth=depth)
        
        # è·å–å¼•ç”¨è¿™ç¯‡è®ºæ–‡çš„æ–‡ç« 
        citing_papers = self._fetch_citations(paper.cited_by_url)
        
        # ä¸ºæ¯ä¸ªå¼•ç”¨è®ºæ–‡é€’å½’æ„å»ºå­æ ‘
        for citing_paper in citing_papers:
            if citing_paper.cited_by_url and depth + 1 < self.max_depth:
                child_node = self._build_citation_subtree(citing_paper, depth + 1)
                if child_node:
                    node.children.append(child_node)
            else:
                # å¶å­èŠ‚ç‚¹
                leaf_node = CitationNode(paper=citing_paper, children=[], depth=depth + 1)
                node.children.append(leaf_node)
        
        return node

    def _update_headers(self):
        """æ›´æ–°è¯·æ±‚å¤´ï¼Œæ¨¡æ‹ŸçœŸå®æµè§ˆå™¨"""
        user_agent = random.choice(self.user_agents)
        self.session.headers.update({
            'User-Agent': user_agent,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0'
        })
        
    def _rotate_proxy(self):
        """è½®æ¢ä»£ç†æœåŠ¡å™¨"""
        if not self.proxy_list:
            return
            
        self.proxy_index = (self.proxy_index + 1) % len(self.proxy_list)
        self.current_proxy = self.proxy_list[self.proxy_index]
        logger.info(f"è½®æ¢ä»£ç†åˆ°: {self.current_proxy}")
        
        # æ›´æ–°sessionä»£ç†
        if self.current_proxy:
            if self.current_proxy.startswith('http'):
                self.session.proxies = {
                    'http': self.current_proxy,
                    'https': self.current_proxy
                }
            else:
                self.session.proxies = {
                    'http': f'socks5://{self.current_proxy}',
                    'https': f'socks5://{self.current_proxy}'
                }
                
        # æ›´æ–°æµè§ˆå™¨ä»£ç†ï¼ˆå¦‚æœæµè§ˆå™¨å·²åˆå§‹åŒ–ï¼‰
        if self.browser:
            try:
                logger.info("æ­£åœ¨å…³é—­æµè§ˆå™¨ä»¥åº”ç”¨æ–°ä»£ç†...")
                self.browser.quit()
                self.browser = None
                logger.info("æµè§ˆå™¨å·²å…³é—­ï¼Œå°†åœ¨ä¸‹æ¬¡è¯·æ±‚æ—¶é‡æ–°åˆå§‹åŒ–")
            except Exception as e:
                logger.error(f"å…³é—­æµè§ˆå™¨æ—¶å‡ºé”™: {e}")
    
    def _is_captcha_page(self, soup: BeautifulSoup) -> bool:
        """æ£€æµ‹æ˜¯å¦é‡åˆ°äº†CAPTCHAé¡µé¢"""
        captcha_indicators = [
            'please show you\'re not a robot',
            'captcha',
            'recaptcha',
            'robot',
            'verify you are human',
            'unusual traffic'
        ]
        
        page_text = soup.get_text().lower()
        for indicator in captcha_indicators:
            if indicator in page_text:
                return True
        
        # æ£€æŸ¥æ˜¯å¦æœ‰reCAPTCHAç›¸å…³çš„å…ƒç´ 
        if soup.find('div', {'class': 'g-recaptcha'}):
            return True
        if soup.find('iframe', src=lambda x: x and 'recaptcha' in x):
            return True
            
        # Additional indicators for CAPTCHA
        additional_indicators = [
            'unusual traffic from your network',
            'automated requests from your computer',
            'network is sending automated queries'
        ]
        for indicator in additional_indicators:
            if indicator in page_text:
                return True
                
        return False
    
    def _handle_captcha_or_block(self, url: str, response_text: str, attempt: int) -> bool:
        """å¤„ç†CAPTCHAæˆ–å°ç¦æƒ…å†µ"""
        logger.warning(f"æ£€æµ‹åˆ°CAPTCHAæˆ–è®¿é—®è¢«é˜»æ­¢: {url} (å°è¯• #{attempt+1})")
        
        # ä¿å­˜è°ƒè¯•ä¿¡æ¯
        debug_file = f"debug_captcha_{int(time.time())}.html"
        with open(debug_file, 'w', encoding='utf-8') as f:
            f.write(response_text)
        logger.info(f"å·²ä¿å­˜è°ƒè¯•é¡µé¢åˆ°: {debug_file}")
        
        # è½®æ¢ä»£ç†
        self._rotate_proxy()
        
        # æŒ‡æ•°é€€é¿å»¶è¿Ÿ
        delay = random.uniform(10, 20) * (1.5 ** attempt)
        logger.info(f"é‡åˆ°CAPTCHAï¼Œç­‰å¾… {delay:.1f} ç§’ (æŒ‡æ•°é€€é¿)...")
        time.sleep(delay)
        
        # æ›´æ–°è¯·æ±‚å¤´
        self._update_headers()
        
        return False
    
    def _adaptive_delay(self):
        """è‡ªé€‚åº”å»¶è¿Ÿç­–ç•¥"""
        self.request_count += 1
        
        # åŸºç¡€å»¶è¿Ÿ
        base_delay = random.uniform(*self.delay_range)
        
        # æ ¹æ®è¯·æ±‚æ¬¡æ•°å¢åŠ å»¶è¿Ÿ
        if self.request_count > 10:
            base_delay *= 1.5
        if self.request_count > 20:
            base_delay *= 2
        
        # å¶å°”ä½¿ç”¨æ›´é•¿çš„å»¶è¿Ÿ
        if random.random() < 0.1:  # 10%æ¦‚ç‡ä½¿ç”¨é•¿å»¶è¿Ÿ
            base_delay *= 2
        
        logger.debug(f"å»¶è¿Ÿ {base_delay:.1f} ç§’ (è¯·æ±‚æ¬¡æ•°: {self.request_count})")
        time.sleep(base_delay)

    def _fetch_with_browser(self, url: str, wait_time=30) -> Optional[str]:
        """ä½¿ç”¨æ— å¤´æµè§ˆå™¨è·å–é¡µé¢å†…å®¹ï¼Œå¯ä»¥ç»•è¿‡ä¸€äº›CAPTCHA"""
        if not BROWSER_AVAILABLE:
            logger.warning("æ— æ³•ä½¿ç”¨æµè§ˆå™¨æ–¹å¼è·å–å†…å®¹ï¼šæµè§ˆå™¨æ¨¡å—æœªåŠ è½½")
            return None
            
        logger.info(f"å°è¯•ä½¿ç”¨æµè§ˆå™¨è·å–é¡µé¢å†…å®¹: {url}")
        
        try:
            if self.browser is None:
                # Set Chrome options to make browser less detectable
                options = uc.ChromeOptions()
                if not self.use_headless_browser:
                    # Only run headless if explicitly enabled
                    options.add_argument('--headless=new')  # Use new headless mode
                options.add_argument('--no-sandbox')
                options.add_argument('--disable-dev-shm-usage')
                options.add_argument('--disable-gpu')
                options.add_argument('--disable-infobars')
                options.add_argument('--disable-extensions')
                options.add_argument('--disable-blink-features=AutomationControlled')
                options.add_argument('--window-size=1280,720')
                options.add_experimental_option("excludeSwitches", ["enable-automation"])
                options.add_experimental_option('useAutomationExtension', False)
                
                # åˆ›å»ºä¸´æ—¶ç”¨æˆ·æ•°æ®ç›®å½•ï¼Œä»¥ä¾¿éš”ç¦»ç¼“å­˜å’ŒCookie
                user_dir = NamedTemporaryFile().name
                options.add_argument(f'--user-data-dir={user_dir}')
                
                # åˆå§‹åŒ–æµè§ˆå™¨
                logger.info("åˆå§‹åŒ–æµè§ˆå™¨...")
                # Set up stealth parameters
                self.browser = uc.Chrome(options=options)
                self.browser.set_page_load_timeout(60)
                self.browser.implicitly_wait(10)
                
                # Execute stealth scripts to avoid detection
                self.browser.execute_script(
                    "Object.defineProperty(navigator, 'webdriver', {get: () => undefined})"
                )
                self.browser.execute_cdp_cmd(
                    'Network.setUserAgentOverride',
                    {'userAgent': random.choice(self.user_agents)}
                )
                logger.info("æµè§ˆå™¨åˆå§‹åŒ–å®Œæˆï¼Œå¯ç”¨äº†åæ£€æµ‹åŠŸèƒ½")
            
            # è®¿é—®é¡µé¢
            logger.info(f"æµè§ˆå™¨æ­£åœ¨æ‰“å¼€: {url}")
            self.browser.get(url)
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            try:
                # ç­‰å¾…é¡µé¢ä¸»ä½“å…ƒç´ åŠ è½½å®Œæˆ
                WebDriverWait(self.browser, wait_time).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, "body"))
                )
                
                # é¢å¤–ç­‰å¾…ï¼Œç¡®ä¿åŠ¨æ€å†…å®¹åŠ è½½
                time.sleep(5)
                
                # è·å–é¡µé¢æºä»£ç 
                page_source = self.browser.page_source
                
                # æ£€æŸ¥æ˜¯å¦ä»ç„¶æœ‰éªŒè¯ç 
                if "recaptcha" in page_source.lower() or "please show you're not a robot" in page_source.lower():
                    if self.captcha_service_api_key:
                        logger.info("å°è¯•ä½¿ç”¨2CaptchaæœåŠ¡è§£å†³éªŒè¯ç ...")
                        solved = self._solve_captcha_with_service()
                        if solved:
                            logger.info("éªŒè¯ç å·²è§£å†³ï¼Œé‡æ–°åŠ è½½é¡µé¢...")
                            self.browser.refresh()
                            time.sleep(5)
                            page_source = self.browser.page_source
                        else:
                            logger.warning("2CaptchaæœåŠ¡æ— æ³•è§£å†³éªŒè¯ç ")
                    else:
                        logger.warning("æµè§ˆå™¨æ–¹å¼ä»ç„¶é‡åˆ°äº†éªŒè¯ç ï¼Œä¸”æœªé…ç½®CAPTCHAæœåŠ¡")
                        # ä¿å­˜æˆªå›¾ä»¥ä¾›è°ƒè¯•
                        screenshot_path = f"browser_captcha_{int(time.time())}.png"
                        self.browser.save_screenshot(screenshot_path)
                        logger.info(f"å·²ä¿å­˜æµè§ˆå™¨æˆªå›¾: {screenshot_path}")
                        return None
                
                logger.info("æµè§ˆå™¨æˆåŠŸè·å–é¡µé¢å†…å®¹")
                return page_source
                
            except TimeoutException:
                logger.error(f"ç­‰å¾…é¡µé¢å…ƒç´ è¶…æ—¶: {url}")
                return None
                
        except WebDriverException as e:
            logger.error(f"æµè§ˆå™¨é‡åˆ°é”™è¯¯: {e}")
            return None
        except Exception as e:
            logger.error(f"ä½¿ç”¨æµè§ˆå™¨è·å–å†…å®¹æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
            logger.error(traceback.format_exc())
            return None
    
    def _close_browser(self):
        """å…³é—­æµè§ˆå™¨å®ä¾‹"""
        if self.browser is not None:
            try:
                logger.info("æ­£åœ¨å…³é—­æµè§ˆå™¨...")
                self.browser.quit()
                self.browser = None
                logger.info("æµè§ˆå™¨å·²å…³é—­")
            except Exception as e:
                logger.error(f"å…³é—­æµè§ˆå™¨æ—¶å‘ç”Ÿé”™è¯¯: {e}")
    
    def _solve_captcha_with_service(self) -> bool:
        """ä½¿ç”¨2CaptchaæœåŠ¡è§£å†³éªŒè¯ç """
        try:
            from twocaptcha import TwoCaptcha
            
            # å¦‚æœä½¿ç”¨ä»£ç†ï¼Œé…ç½®2Captchaä½¿ç”¨ç›¸åŒä»£ç†
            solver_config = {'apiKey': self.captcha_service_api_key}
            if self.current_proxy:
                solver_config['defaultTimeout'] = 120
                solver_config['recaptchaTimeout'] = 600
                solver_config['pollingInterval'] = 10
                
                if self.current_proxy.startswith('http'):
                    solver_config['proxy'] = {
                        'type': 'HTTP',
                        'uri': self.current_proxy
                    }
                else:
                    solver_config['proxy'] = {
                        'type': 'SOCKS5',
                        'uri': self.current_proxy
                    }
            
            solver = TwoCaptcha(**solver_config)
            
            if not self.captcha_service_api_key:
                logger.warning("æœªé…ç½®2Captcha APIå¯†é’¥ï¼Œæ— æ³•ä½¿ç”¨éªŒè¯ç è§£å†³æœåŠ¡")
                return False
                
            # æå–reCAPTCHA sitekey
            sitekey = self.browser.find_element(By.CSS_SELECTOR, '[data-sitekey]').get_attribute('data-sitekey')
            if not sitekey:
                logger.error("æ— æ³•åœ¨é¡µé¢ä¸Šæ‰¾åˆ°reCAPTCHA sitekey")
                return False
                
            # è·å–å½“å‰URL
            page_url = self.browser.current_url
            
            # åˆ›å»º2Captchaæ±‚è§£å™¨
            solver = TwoCaptcha(self.captcha_service_api_key)
            
            try:
                logger.info("æ­£åœ¨å‘2Captchaå‘é€éªŒè¯ç æ±‚è§£è¯·æ±‚...")
                result = solver.recaptcha(sitekey, page_url)
                logger.info(f"2Captchaè¿”å›ç»“æœ: {result}")
                
                # å°†è§£å†³æ–¹æ¡ˆæ³¨å…¥é¡µé¢
                self.browser.execute_script(
                    f"document.getElementById('g-recaptcha-response').innerHTML = '{result['code']}';"
                )
                time.sleep(1)
                
                # æäº¤éªŒè¯ç è¡¨å•
                submit_button = self.browser.find_element(By.CSS_SELECTOR, 'button[type="submit"]')
                submit_button.click()
                time.sleep(5)
                
                logger.info("éªŒè¯ç è§£å†³æ–¹æ¡ˆå·²æäº¤")
                return True
                
            except Exception as e:
                logger.error(f"2CaptchaæœåŠ¡é”™è¯¯: {e}")
                return False
                
        except ImportError:
            logger.error("æœªå®‰è£…2Captcha Pythonåº“ï¼Œè¯·è¿è¡Œ: pip install 2captcha-python")
            return False
        except Exception as e:
            logger.error(f"è§£å†³éªŒè¯ç æ—¶å‡ºé”™: {e}")
            return False
            
    def __del__(self):
        """ææ„å‡½æ•°ï¼Œç¡®ä¿é‡Šæ”¾æµè§ˆå™¨èµ„æº"""
        self._close_browser()

def print_citation_tree(node: CitationNode, indent: str = ""):
    """æ‰“å°å¼•ç”¨æ ‘"""
    print(f"{indent}ğŸ“„ {node.paper.title}")
    print(f"{indent}   ğŸ‘¥ {node.paper.authors}")
    if node.paper.year:
        print(f"{indent}   ğŸ“… {node.paper.year}")
    if node.paper.citation_count > 0:
        print(f"{indent}   ğŸ“Š è¢«å¼•ç”¨ {node.paper.citation_count} æ¬¡")
    print()
    
    for child in node.children:
        print_citation_tree(child, indent + "  ")

def save_tree_to_json(node: CitationNode, filename: str):
    """å°†å¼•ç”¨æ ‘ä¿å­˜ä¸ºJSONæ–‡ä»¶"""
    def node_to_dict(n: CitationNode) -> dict:
        return {
            "paper": {
                "title": n.paper.title,
                "authors": n.paper.authors,
                "year": n.paper.year,
                "citation_count": n.paper.citation_count,
                "url": n.paper.url,
                "cited_by_url": n.paper.cited_by_url,
                "abstract": n.paper.abstract
            },
            "depth": n.depth,
            "children": [node_to_dict(child) for child in n.children]
        }
    
    tree_dict = node_to_dict(node)
    
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(tree_dict, f, ensure_ascii=False, indent=2)
    
    logger.info(f"å¼•ç”¨æ ‘å·²ä¿å­˜åˆ°: {filename}")

# ç¤ºä¾‹ä½¿ç”¨
sample_link = "https://scholar.google.com/scholar?cites=11002616430871081935&as_sdt=2005&sciodt=0,5&hl=en"

def main():
    """ä¸»å‡½æ•°"""
    crawler = GoogleScholarCrawler(
        max_depth=10,  # æœ€å¤§é€’å½’æ·±åº¦
        max_papers_per_level=30,  # æ¯å±‚æœ€å¤šçˆ¬å–çš„è®ºæ–‡æ•°
        delay_range=(1, 3)  # è¯·æ±‚é—´éš”æ—¶é—´èŒƒå›´ï¼ˆç§’ï¼‰
    )
    
    print("ğŸš€ å¼€å§‹æ„å»ºå¼•ç”¨æ ‘...")
    print(f"ğŸ“‹ èµ·å§‹é“¾æ¥: {sample_link}")
    print(f"ğŸ“Š æœ€å¤§æ·±åº¦: {crawler.max_depth}")
    print(f"ğŸ“ˆ æ¯å±‚æœ€å¤šè®ºæ–‡æ•°: {crawler.max_papers_per_level}")
    print("-" * 50)
    
    # æ„å»ºå¼•ç”¨æ ‘
    citation_tree = crawler.build_citation_tree(sample_link)
    
    if citation_tree:
        print("\nâœ… å¼•ç”¨æ ‘æ„å»ºå®Œæˆ!")
        print("=" * 50)
        print_citation_tree(citation_tree)
        
        # ä¿å­˜ä¸ºJSONæ–‡ä»¶
        save_tree_to_json(citation_tree, "citation_tree.json")
        
        # ç»Ÿè®¡ä¿¡æ¯
        def count_nodes(node):
            count = 1
            for child in node.children:
                count += count_nodes(child)
            return count
        
        total_papers = count_nodes(citation_tree)
        print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"   æ€»è®ºæ–‡æ•°: {total_papers}")
        print(f"   æ ‘çš„æ·±åº¦: {citation_tree.depth}")
        print(f"   æ ¹èŠ‚ç‚¹å­èŠ‚ç‚¹æ•°: {len(citation_tree.children)}")
        
    else:
        print("âŒ æ— æ³•æ„å»ºå¼•ç”¨æ ‘")

if __name__ == "__main__":
    main()
