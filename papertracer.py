"""
Google Scholar å¼•ç”¨çˆ¬è™«
é€’å½’çˆ¬å–æ–‡ç« çš„å¼•ç”¨å…³ç³»å¹¶è¿”å›æ ‘å½¢æ•°æ®ç»“æ„
"""

import requests
from bs4 import BeautifulSoup
import time
import random
import re
from urllib.parse import urljoin, parse_qs, urlparse
from dataclasses import dataclass
from typing import List, Dict, Optional, Set
import json
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class Paper:
    """è®ºæ–‡æ•°æ®ç»“æ„"""
    title: str
    authors: str = ""
    year: str = ""
    citation_count: int = 0
    url: str = ""
    cited_by_url: str = ""
    abstract: str = ""

@dataclass 
class CitationNode:
    """å¼•ç”¨æ ‘èŠ‚ç‚¹"""
    paper: Paper
    children: List['CitationNode']
    depth: int = 0

class GoogleScholarCrawler:
    """Google Scholar çˆ¬è™«ç±»"""
    
    def __init__(self, max_depth=3, max_papers_per_level=10, delay_range=(1, 3)):
        self.max_depth = max_depth
        self.max_papers_per_level = max_papers_per_level
        self.delay_range = delay_range
        self.visited_urls: Set[str] = set()
        self.session = requests.Session()
        
        # è®¾ç½®User-Agenté¿å…è¢«å°
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
    
    def _get_random_delay(self):
        """è·å–éšæœºå»¶è¿Ÿæ—¶é—´"""
        return random.uniform(*self.delay_range)
    
    def _extract_cluster_id(self, url: str) -> Optional[str]:
        """ä»URLä¸­æå–cluster ID"""
        try:
            parsed = urlparse(url)
            params = parse_qs(parsed.query)
            if 'cites' in params:
                return params['cites'][0]
            elif 'cluster' in params:
                return params['cluster'][0]
        except:
            pass
        return None
    
    def _parse_paper_info(self, result_div) -> Paper:
        """è§£æå•ç¯‡è®ºæ–‡ä¿¡æ¯"""
        try:
            # æå–æ ‡é¢˜
            title_elem = result_div.find('h3', class_='gs_rt')
            title = title_elem.get_text(strip=True) if title_elem else "Unknown Title"
            
            # æå–ä½œè€…å’Œå¹´ä»½
            authors_elem = result_div.find('div', class_='gs_a')
            authors_text = authors_elem.get_text(strip=True) if authors_elem else ""
            
            # å°è¯•æå–å¹´ä»½
            year_match = re.search(r'\b(19|20)\d{2}\b', authors_text)
            year = year_match.group() if year_match else ""
            
            # æå–ä½œè€…ï¼ˆå¹´ä»½å‰çš„éƒ¨åˆ†ï¼‰
            if year:
                authors = authors_text.split(year)[0].strip(' -,')
            else:
                authors = authors_text
            
            # æå–å¼•ç”¨æ¬¡æ•°
            cite_elem = result_div.find('a', string=re.compile(r'Cited by \d+'))
            citation_count = 0
            cited_by_url = ""
            
            if cite_elem:
                cite_text = cite_elem.get_text(strip=True)
                cite_match = re.search(r'Cited by (\d+)', cite_text)
                if cite_match:
                    citation_count = int(cite_match.group(1))
                    cited_by_url = urljoin('https://scholar.google.com', cite_elem.get('href', ''))
            
            # æå–è®ºæ–‡URL
            paper_url = ""
            if title_elem and title_elem.find('a'):
                paper_url = title_elem.find('a').get('href', '')
            
            # æå–æ‘˜è¦
            abstract_elem = result_div.find('span', class_='gs_rs')
            abstract = abstract_elem.get_text(strip=True) if abstract_elem else ""
            
            return Paper(
                title=title,
                authors=authors,
                year=year,
                citation_count=citation_count,
                url=paper_url,
                cited_by_url=cited_by_url,
                abstract=abstract
            )
        
        except Exception as e:
            logger.error(f"è§£æè®ºæ–‡ä¿¡æ¯æ—¶å‡ºé”™: {e}")
            return Paper(title="Parse Error", authors="", year="")
    
    def _fetch_citations(self, cited_by_url: str) -> List[Paper]:
        """è·å–å¼•ç”¨è¯¥è®ºæ–‡çš„æ–‡ç« åˆ—è¡¨"""
        if not cited_by_url or cited_by_url in self.visited_urls:
            return []
        
        self.visited_urls.add(cited_by_url)
        
        try:
            logger.info(f"æ­£åœ¨çˆ¬å–: {cited_by_url}")
            
            # éšæœºå»¶è¿Ÿ
            time.sleep(self._get_random_delay())
            
            response = self.session.get(cited_by_url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # æŸ¥æ‰¾æ‰€æœ‰è®ºæ–‡ç»“æœ
            paper_divs = soup.find_all('div', class_='gs_r')
            papers = []
            
            for div in paper_divs[:self.max_papers_per_level]:
                paper = self._parse_paper_info(div)
                if paper.title != "Parse Error":
                    papers.append(paper)
            
            logger.info(f"æ‰¾åˆ° {len(papers)} ç¯‡å¼•ç”¨è®ºæ–‡")
            return papers
            
        except requests.RequestException as e:
            logger.error(f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}")
            return []
        except Exception as e:
            logger.error(f"è§£æé¡µé¢å¤±è´¥: {e}")
            return []
    
    def _get_paper_from_scholar_url(self, scholar_url: str) -> Optional[Paper]:
        """ä»Scholaræœç´¢URLè·å–åŸå§‹è®ºæ–‡ä¿¡æ¯"""
        try:
            logger.info(f"è·å–åŸå§‹è®ºæ–‡ä¿¡æ¯: {scholar_url}")
            
            response = self.session.get(scholar_url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # æŸ¥æ‰¾ç¬¬ä¸€ä¸ªæœç´¢ç»“æœ
            first_result = soup.find('div', class_='gs_r')
            if first_result:
                return self._parse_paper_info(first_result)
            
        except Exception as e:
            logger.error(f"è·å–åŸå§‹è®ºæ–‡ä¿¡æ¯å¤±è´¥: {e}")
        
        return None
    
    def build_citation_tree(self, start_url: str, current_depth: int = 0) -> Optional[CitationNode]:
        """é€’å½’æ„å»ºå¼•ç”¨æ ‘"""
        if current_depth >= self.max_depth:
            return None
        
        logger.info(f"æ„å»ºå¼•ç”¨æ ‘ï¼Œæ·±åº¦: {current_depth}")
        
        # å¦‚æœæ˜¯èµ·å§‹URLï¼Œéœ€è¦å…ˆè·å–åŸå§‹è®ºæ–‡ä¿¡æ¯
        if current_depth == 0:
            if 'cites=' in start_url:
                # è¿™æ˜¯ä¸€ä¸ª"è¢«å¼•ç”¨"é¡µé¢ï¼Œéœ€è¦è·å–åŸå§‹è®ºæ–‡ä¿¡æ¯
                # æ„é€ ä¸€ä¸ªè™šæ‹Ÿçš„æ ¹è®ºæ–‡
                root_paper = Paper(
                    title="Root Paper (ä»å¼•ç”¨é¡µé¢å¼€å§‹)",
                    authors="Unknown",
                    year="",
                    citation_count=0,
                    url="",
                    cited_by_url=start_url
                )
            else:
                # å°è¯•ä»æœç´¢ç»“æœè·å–è®ºæ–‡ä¿¡æ¯
                root_paper = self._get_paper_from_scholar_url(start_url)
                if not root_paper:
                    logger.error("æ— æ³•è·å–èµ·å§‹è®ºæ–‡ä¿¡æ¯")
                    return None
        else:
            # è¿™ç§æƒ…å†µåœ¨é€’å½’è°ƒç”¨ä¸­ä¸åº”è¯¥å‘ç”Ÿ
            return None
        
        # åˆ›å»ºæ ¹èŠ‚ç‚¹
        root_node = CitationNode(paper=root_paper, children=[], depth=current_depth)
        
        # è·å–å¼•ç”¨è¿™ç¯‡è®ºæ–‡çš„æ–‡ç« 
        citing_papers = self._fetch_citations(root_paper.cited_by_url)
        
        # ä¸ºæ¯ä¸ªå¼•ç”¨è®ºæ–‡é€’å½’æ„å»ºå­æ ‘
        for citing_paper in citing_papers:
            if citing_paper.cited_by_url and current_depth + 1 < self.max_depth:
                child_node = self._build_citation_subtree(citing_paper, current_depth + 1)
                if child_node:
                    root_node.children.append(child_node)
            else:
                # å¶å­èŠ‚ç‚¹
                leaf_node = CitationNode(paper=citing_paper, children=[], depth=current_depth + 1)
                root_node.children.append(leaf_node)
        
        return root_node
    
    def _build_citation_subtree(self, paper: Paper, depth: int) -> Optional[CitationNode]:
        """é€’å½’æ„å»ºå¼•ç”¨å­æ ‘"""
        if depth >= self.max_depth:
            return None
        
        node = CitationNode(paper=paper, children=[], depth=depth)
        
        # è·å–å¼•ç”¨è¿™ç¯‡è®ºæ–‡çš„æ–‡ç« 
        citing_papers = self._fetch_citations(paper.cited_by_url)
        
        # ä¸ºæ¯ä¸ªå¼•ç”¨è®ºæ–‡é€’å½’æ„å»ºå­æ ‘
        for citing_paper in citing_papers:
            if citing_paper.cited_by_url and depth + 1 < self.max_depth:
                child_node = self._build_citation_subtree(citing_paper, depth + 1)
                if child_node:
                    node.children.append(child_node)
            else:
                # å¶å­èŠ‚ç‚¹
                leaf_node = CitationNode(paper=citing_paper, children=[], depth=depth + 1)
                node.children.append(leaf_node)
        
        return node

def print_citation_tree(node: CitationNode, indent: str = ""):
    """æ‰“å°å¼•ç”¨æ ‘"""
    print(f"{indent}ğŸ“„ {node.paper.title}")
    print(f"{indent}   ğŸ‘¥ {node.paper.authors}")
    if node.paper.year:
        print(f"{indent}   ğŸ“… {node.paper.year}")
    if node.paper.citation_count > 0:
        print(f"{indent}   ğŸ“Š è¢«å¼•ç”¨ {node.paper.citation_count} æ¬¡")
    print()
    
    for child in node.children:
        print_citation_tree(child, indent + "  ")

def save_tree_to_json(node: CitationNode, filename: str):
    """å°†å¼•ç”¨æ ‘ä¿å­˜ä¸ºJSONæ–‡ä»¶"""
    def node_to_dict(n: CitationNode) -> dict:
        return {
            "paper": {
                "title": n.paper.title,
                "authors": n.paper.authors,
                "year": n.paper.year,
                "citation_count": n.paper.citation_count,
                "url": n.paper.url,
                "cited_by_url": n.paper.cited_by_url,
                "abstract": n.paper.abstract
            },
            "depth": n.depth,
            "children": [node_to_dict(child) for child in n.children]
        }
    
    tree_dict = node_to_dict(node)
    
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(tree_dict, f, ensure_ascii=False, indent=2)
    
    logger.info(f"å¼•ç”¨æ ‘å·²ä¿å­˜åˆ°: {filename}")

# ç¤ºä¾‹ä½¿ç”¨
sample_link = "https://scholar.google.com/scholar?cites=11002616430871081935&as_sdt=2005&sciodt=0,5&hl=en"

def main():
    """ä¸»å‡½æ•°"""
    crawler = GoogleScholarCrawler(
        max_depth=3,  # æœ€å¤§é€’å½’æ·±åº¦
        max_papers_per_level=5,  # æ¯å±‚æœ€å¤šçˆ¬å–çš„è®ºæ–‡æ•°
        delay_range=(1, 3)  # è¯·æ±‚é—´éš”æ—¶é—´èŒƒå›´ï¼ˆç§’ï¼‰
    )
    
    print("ğŸš€ å¼€å§‹æ„å»ºå¼•ç”¨æ ‘...")
    print(f"ğŸ“‹ èµ·å§‹é“¾æ¥: {sample_link}")
    print(f"ğŸ“Š æœ€å¤§æ·±åº¦: {crawler.max_depth}")
    print(f"ğŸ“ˆ æ¯å±‚æœ€å¤šè®ºæ–‡æ•°: {crawler.max_papers_per_level}")
    print("-" * 50)
    
    # æ„å»ºå¼•ç”¨æ ‘
    citation_tree = crawler.build_citation_tree(sample_link)
    
    if citation_tree:
        print("\nâœ… å¼•ç”¨æ ‘æ„å»ºå®Œæˆ!")
        print("=" * 50)
        print_citation_tree(citation_tree)
        
        # ä¿å­˜ä¸ºJSONæ–‡ä»¶
        save_tree_to_json(citation_tree, "citation_tree.json")
        
        # ç»Ÿè®¡ä¿¡æ¯
        def count_nodes(node):
            count = 1
            for child in node.children:
                count += count_nodes(child)
            return count
        
        total_papers = count_nodes(citation_tree)
        print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"   æ€»è®ºæ–‡æ•°: {total_papers}")
        print(f"   æ ‘çš„æ·±åº¦: {citation_tree.depth}")
        print(f"   æ ¹èŠ‚ç‚¹å­èŠ‚ç‚¹æ•°: {len(citation_tree.children)}")
        
    else:
        print("âŒ æ— æ³•æ„å»ºå¼•ç”¨æ ‘")

if __name__ == "__main__":
    main()
