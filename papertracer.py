"""
Google Scholar å¼•ç”¨çˆ¬è™«
é€’å½’çˆ¬å–æ–‡ç« çš„å¼•ç”¨å…³ç³»å¹¶è¿”å›æ ‘å½¢æ•°æ®ç»“æ„
"""

import requests
from bs4 import BeautifulSoup
import time
import random
import re
from urllib.parse import urljoin, parse_qs, urlparse
from dataclasses import dataclass
from typing import List, Dict, Optional, Set
import json
import logging
import platform
from pathlib import Path
from tempfile import NamedTemporaryFile
import traceback
import pickle
from datetime import datetime, timedelta

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# å°è¯•å¯¼å…¥æ— å¤´æµè§ˆå™¨ç›¸å…³ä¾èµ–ï¼ˆå¯é€‰çš„ï¼Œç”¨äºå¤„ç†CAPTCHAï¼‰
BROWSER_AVAILABLE = False
try:
    import undetected_chromedriver as uc
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.common.exceptions import TimeoutException, WebDriverException
    BROWSER_AVAILABLE = True
    logger.info("æµè§ˆå™¨æ¨¡å—å·²åŠ è½½ï¼Œå¯ç”¨äºå¤„ç†CAPTCHA")
except ImportError:
    logger.warning("æµè§ˆå™¨æ¨¡å—å¯¼å…¥å¤±è´¥ï¼Œæ— æ³•ä½¿ç”¨æµè§ˆå™¨ç»•è¿‡CAPTCHAã€‚è€ƒè™‘å®‰è£…: pip install undetected-chromedriver selenium")

@dataclass
class Paper:
    """è®ºæ–‡æ•°æ®ç»“æ„"""
    title: str
    authors: str = ""
    year: str = ""
    citation_count: int = 0
    url: str = ""
    cited_by_url: str = ""
    abstract: str = ""

@dataclass 
class CitationNode:
    """å¼•ç”¨æ ‘èŠ‚ç‚¹"""
    paper: Paper
    children: List['CitationNode']
    depth: int = 0

class GoogleScholarCrawler:
    """Google Scholar çˆ¬è™«ç±»"""
    
    def __init__(self, max_depth=3, max_papers_per_level=10, delay_range=(2, 5), max_captcha_retries=3,
                 use_browser_fallback=True, captcha_service_api_key=None, proxy_list=None, skip_429_errors=False):
        self.max_depth = max_depth
        self.max_papers_per_level = max_papers_per_level
        self.delay_range = delay_range
        self.max_captcha_retries = max_captcha_retries
        self.use_browser_fallback = use_browser_fallback and BROWSER_AVAILABLE
        self.use_headless_browser = True  # æ·»åŠ ç¼ºå¤±çš„å±æ€§
        self.captcha_service_api_key = captcha_service_api_key
        self.proxy_list = proxy_list or []
        self.skip_429_errors = skip_429_errors  # æ–°å¢: æ˜¯å¦è·³è¿‡429é”™è¯¯
        self.proxy_index = 0
        self.current_proxy = self.proxy_list[0] if self.proxy_list else None
        self.visited_urls: Set[str] = set()
        self.session = requests.Session()
        self.user_agents = [
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0'
        ]
        self.request_count = 0
        self.browser = None
        
        # Session persistence
        self.session_id = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.last_429_time = None
        self.consecutive_429_count = 0
        
        # è®¾ç½®æ›´å®Œæ•´çš„è¯·æ±‚å¤´
        self._update_headers()

    def _get_random_delay(self):
        """è·å–éšæœºå»¶è¿Ÿæ—¶é—´ï¼ˆå·²åºŸå¼ƒï¼Œä½¿ç”¨_adaptive_delayä»£æ›¿ï¼‰"""
        return random.uniform(*self.delay_range)
    
    def _extract_cluster_id(self, url: str) -> Optional[str]:
        """ä»URLä¸­æå–cluster ID"""
        try:
            parsed = urlparse(url)
            params = parse_qs(parsed.query)
            if 'cites' in params:
                return params['cites'][0]
            elif 'cluster' in params:
                return params['cluster'][0]
        except:
            pass
        return None
    
    def _parse_paper_info(self, result_div) -> Paper:
        """è§£æå•ç¯‡è®ºæ–‡ä¿¡æ¯"""
        try:
            # æå–æ ‡é¢˜ - å°è¯•å¤šç§é€‰æ‹©å™¨
            title_elem = (result_div.find('h3', class_='gs_rt') or 
                         result_div.find('h3') or 
                         result_div.find('a', class_='gs_rt'))
            
            if title_elem:
                # å¦‚æœæ ‡é¢˜åœ¨é“¾æ¥å†…ï¼Œæå–é“¾æ¥æ–‡æœ¬
                title_link = title_elem.find('a') if title_elem.name != 'a' else title_elem
                if title_link:
                    title = title_link.get_text(strip=True)
                else:
                    title = title_elem.get_text(strip=True)
            else:
                title = "Unknown Title"
            
            # è¿‡æ»¤æ‰æ˜æ˜¾çš„é”™è¯¯æ ‡é¢˜
            if not title or title.lower() in ['unknown title', 'parse error', '']:
                logger.debug("å‘ç°ç©ºæ ‡é¢˜æˆ–é”™è¯¯æ ‡é¢˜ï¼Œè·³è¿‡")
                return Paper(title="Parse Error", authors="", year="")
            
            # æå–ä½œè€…å’Œå¹´ä»½
            authors_elem = result_div.find('div', class_='gs_a')
            authors_text = authors_elem.get_text(strip=True) if authors_elem else ""
            
            # å°è¯•æå–å¹´ä»½
            year_match = re.search(r'\b(19|20)\d{2}\b', authors_text)
            year = year_match.group() if year_match else ""
            
            # æå–ä½œè€…ï¼ˆå¹´ä»½å‰çš„éƒ¨åˆ†ï¼‰
            if year:
                authors = authors_text.split(year)[0].strip(' -,')
            else:
                authors = authors_text
            
            # æ¸…ç†ä½œè€…ä¿¡æ¯
            authors = re.sub(r'\s*-\s*$', '', authors)  # ç§»é™¤æœ«å°¾çš„ç ´æŠ˜å·
            authors = re.sub(r'\s+', ' ', authors)  # æ ‡å‡†åŒ–ç©ºæ ¼
            
            # æå–å¼•ç”¨æ¬¡æ•°
            cite_elem = result_div.find('a', string=re.compile(r'Cited by \d+', re.IGNORECASE))
            if not cite_elem:
                # å°è¯•å…¶ä»–å¯èƒ½çš„å¼•ç”¨é“¾æ¥æ ¼å¼
                cite_elem = result_div.find('a', href=re.compile(r'cites='))
            
            citation_count = 0
            cited_by_url = ""
            
            if cite_elem:
                cite_text = cite_elem.get_text(strip=True)
                cite_match = re.search(r'Cited by (\d+)', cite_text, re.IGNORECASE)
                if cite_match:
                    citation_count = int(cite_match.group(1))
                    cited_by_url = urljoin('https://scholar.google.com', cite_elem.get('href', ''))
            
            # æå–è®ºæ–‡URL
            paper_url = ""
            if title_elem:
                link_elem = title_elem.find('a') if title_elem.name != 'a' else title_elem
                if link_elem:
                    paper_url = link_elem.get('href', '')
            
            # æå–æ‘˜è¦
            abstract_elem = result_div.find('span', class_='gs_rs')
            abstract = abstract_elem.get_text(strip=True) if abstract_elem else ""
            
            paper = Paper(
                title=title,
                authors=authors,
                year=year,
                citation_count=citation_count,
                url=paper_url,
                cited_by_url=cited_by_url,
                abstract=abstract
            )
            
            logger.debug(f"è§£æè®ºæ–‡æˆåŠŸ: {title[:50]}...")
            return paper
        
        except Exception as e:
            logger.error(f"è§£æè®ºæ–‡ä¿¡æ¯æ—¶å‡ºé”™: {e}")
            return Paper(title="Parse Error", authors="", year="")
    
    def _make_request(self, url: str, timeout: int = 20) -> Optional[requests.Response]:
        """ç»Ÿä¸€çš„è¯·æ±‚æ–¹æ³•ï¼Œè‡ªåŠ¨é€‰æ‹©ScrapingAntä»£ç†æ± æˆ–å¸¸è§„è¯·æ±‚"""
        # å¸¸è§„è¯·æ±‚æ–¹æ³•
        self._adaptive_delay()
        
        if self.request_count % 5 == 0:
            self._update_headers()
        
        response = self.session.get(url, timeout=timeout)
        response.raise_for_status()
        return response

    def _fetch_citations(self, cited_by_url: str) -> List[Paper]:
        """è·å–å¼•ç”¨è¯¥è®ºæ–‡çš„æ–‡ç« åˆ—è¡¨"""
        if not cited_by_url or cited_by_url in self.visited_urls:
            return []
        
        self.visited_urls.add(cited_by_url)  # Mark as visited once we start processing it
        
        attempt = 0
        browser_attempt = False  # æ ‡è®°æ˜¯å¦å·²å°è¯•ä½¿ç”¨æµè§ˆå™¨
        
        while attempt < self.max_captcha_retries:
            try:
                logger.info(f"æ­£åœ¨çˆ¬å–: {cited_by_url} (å°è¯• {attempt + 1}/{self.max_captcha_retries})")
                
                # ä½¿ç”¨æµè§ˆå™¨fallbackå°è¯•ç»•è¿‡CAPTCHA
                if browser_attempt and self.use_browser_fallback:
                    logger.info("å°è¯•ä½¿ç”¨æµè§ˆå™¨æ–¹å¼ç»•è¿‡CAPTCHA...")
                    html_content = self._fetch_with_browser(cited_by_url)
                    if html_content:
                        soup = BeautifulSoup(html_content, 'html.parser')
                    else:
                        logger.warning("æµè§ˆå™¨è·å–å†…å®¹å¤±è´¥ï¼Œè¿”å›ç©ºç»“æœ")
                        return []
                else:
                    # å¸¸è§„è¯·æ±‚æ–¹æ³•
                    self._adaptive_delay()
                    
                    if self.request_count % 5 == 0:
                        self._update_headers()
                    
                    response = self.session.get(cited_by_url, timeout=20)
                    response.raise_for_status()
                    
                    soup = BeautifulSoup(response.content, 'html.parser')
                
                # æ£€æµ‹CAPTCHAæˆ–å°ç¦
                if self._is_captcha_page(soup):
                    logger.warning(f"CAPTCHA æ£€æµ‹äº: {cited_by_url} (å°è¯• {attempt + 1})")
                    
                    # æ‰§è¡ŒCAPTCHAå¤„ç†ç­–ç•¥
                    logger.info("ğŸ”„ æ‰§è¡ŒCAPTCHAå¤„ç†ç­–ç•¥...")
                    
                    # 1. æ›´æ–°è¯·æ±‚å¤´å’Œå¢åŠ éšæœºæ€§
                    self._update_headers()
                    logger.info("   âœ“ å·²æ›´æ–°è¯·æ±‚å¤´")
                    
                    # 2. ä½¿ç”¨æ¸è¿›å¼å»¶è¿Ÿç­–ç•¥
                    retry_delay = 2 + attempt * 2 + random.uniform(1, 3)
                    logger.info(f"   âœ“ æ‰§è¡Œæ¸è¿›å¼å»¶è¿Ÿé‡è¯•: {retry_delay:.1f} ç§’")
                    time.sleep(retry_delay)
                    
                    # 3. å¦‚æœå¯ç”¨äº†429è·³è¿‡æ¨¡å¼ï¼Œä¸ä½¿ç”¨æµè§ˆå™¨å¤„ç†ä½†ç»§ç»­å°è¯•è‡ªåŠ¨ç­–ç•¥
                    if self.skip_429_errors:
                        # æ·»åŠ é¢å¤–éšæœºå»¶è¿Ÿä»¥å¢åŠ ä¸‹æ¬¡æˆåŠŸæ¦‚ç‡
                        if attempt > 0:
                            extra_delay = random.uniform(3, 8)
                            logger.info(f"   âœ“ æ‰§è¡Œé¢å¤–å»¶è¿Ÿ: {extra_delay:.1f} ç§’")
                            time.sleep(extra_delay)
                            
                        logger.info("â­ï¸  è·³è¿‡æ¨¡å¼å·²å¯ç”¨ï¼Œè·³è¿‡æµè§ˆå™¨CAPTCHAå¤„ç†")
                        logger.info("   âœ“ å·²æ‰§è¡Œæ‰€æœ‰è‡ªåŠ¨åŒ–ç­–ç•¥ï¼Œç»§ç»­å°è¯•")
                        attempt += 1
                        continue  # ç»§ç»­ä¸‹ä¸€æ¬¡å°è¯•
                    
                    # 4. é»˜è®¤æ¨¡å¼ï¼šå¦‚æœè¿˜æ²¡å°è¯•è¿‡æµè§ˆå™¨æ–¹æ³•ï¼Œä¸”é…ç½®å…è®¸ï¼Œåˆ™å°è¯•æµè§ˆå™¨æ–¹æ³•
                    if not browser_attempt and self.use_browser_fallback:
                        logger.info("æ£€æµ‹åˆ°CAPTCHAï¼Œåˆ‡æ¢åˆ°æµè§ˆå™¨æ–¹å¼å°è¯•...")
                        browser_attempt = True
                        # ä¸å¢åŠ å°è¯•æ¬¡æ•°ï¼Œç›´æ¥è¿›å…¥ä¸‹ä¸€å¾ªç¯ç”¨æµè§ˆå™¨è®¿é—®
                        continue
                    
                    # å·²ç»å°è¯•è¿‡æµè§ˆå™¨æˆ–ä¸å…è®¸ä½¿ç”¨æµè§ˆå™¨ï¼Œåˆ™å¤„ç†CAPTCHA
                    self._handle_captcha_or_block(cited_by_url, response.text if 'response' in locals() else "", attempt)
                    attempt += 1
                    
                    if attempt >= self.max_captcha_retries:
                        logger.error(f"CAPTCHA éªŒè¯å¤±è´¥æ¬¡æ•°è¿‡å¤šï¼Œæ”¾å¼ƒçˆ¬å–: {cited_by_url}")
                        return []
                    
                    logger.info(f"CAPTCHA åç­‰å¾…åé‡è¯• ({attempt + 1}/{self.max_captcha_retries}): {cited_by_url}")
                    browser_attempt = False  # é‡ç½®æµè§ˆå™¨å°è¯•çŠ¶æ€ï¼Œå†æ¬¡ä»å¸¸è§„è¯·æ±‚å¼€å§‹
                    continue
                
                # æŸ¥æ‰¾æ‰€æœ‰è®ºæ–‡ç»“æœ
                paper_divs = soup.find_all('div', class_='gs_r')
                
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç»“æœï¼Œå¯èƒ½æ˜¯é¡µé¢ç»“æ„å‘ç”Ÿäº†å˜åŒ–
                if not paper_divs:
                    logger.warning(f"æœªæ‰¾åˆ°è®ºæ–‡ç»“æœï¼Œå¯èƒ½é¡µé¢ç»“æ„å·²å˜åŒ–: {cited_by_url}")
                    # å°è¯•å…¶ä»–å¯èƒ½çš„é€‰æ‹©å™¨
                    paper_divs = soup.find_all('div', class_='gs_ri') or soup.find_all('div', {'data-lid': True})
                
                papers = []
                
                for div in paper_divs[:self.max_papers_per_level]:
                    paper = self._parse_paper_info(div)
                    if paper.title != "Parse Error" and paper.title != "Unknown Title":
                        papers.append(paper)
                
                # æŒ‰å¼•ç”¨æ¬¡æ•°æ’åºï¼ˆé™åºï¼‰ï¼Œå¼•ç”¨æ¬¡æ•°é«˜çš„è®ºæ–‡ä¼˜å…ˆ
                papers.sort(key=lambda p: p.citation_count, reverse=True)
                logger.info(f"æ‰¾åˆ° {len(papers)} ç¯‡æœ‰æ•ˆå¼•ç”¨è®ºæ–‡ï¼Œå·²æŒ‰å¼•ç”¨é‡æ’åº from {cited_by_url}")
                if papers:
                    logger.info(f"å¼•ç”¨é‡èŒƒå›´: {papers[0].citation_count} åˆ° {papers[-1].citation_count}")
                
                # æˆåŠŸè¯·æ±‚ï¼Œé‡ç½®429è·Ÿè¸ª
                self._reset_429_tracking()
                
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•æœ‰æ•ˆè®ºæ–‡ï¼Œè®°å½•è°ƒè¯•ä¿¡æ¯
                if not papers and paper_divs:
                    debug_file = f"debug_no_papers_{int(time.time())}.html"
                    with open(debug_file, 'w', encoding='utf-8') as f:
                        f.write(soup.prettify())
                    logger.warning(f"è§£ææˆåŠŸä½†æœªæå–åˆ°è®ºæ–‡ï¼Œå·²ä¿å­˜è°ƒè¯•é¡µé¢åˆ°: {debug_file}")
                
                return papers  # Success
            
            except requests.exceptions.RequestException as e:
                error_msg = str(e)
                logger.error(f"ç½‘ç»œè¯·æ±‚å¤±è´¥ ({cited_by_url}): {error_msg} (å°è¯• {attempt + 1}/{self.max_captcha_retries})")
                
                # ç‰¹æ®Šå¤„ç†429é”™è¯¯ - Too Many Requests
                if "429" in error_msg or "Too Many Requests" in error_msg:
                    result = self._handle_429_error(cited_by_url)
                    if result:  # å¦‚æœæ‰‹åŠ¨éªŒè¯æˆåŠŸï¼Œä½¿ç”¨è¿”å›çš„é¡µé¢å†…å®¹
                        soup = BeautifulSoup(result, 'html.parser')
                        # ç»§ç»­æ­£å¸¸çš„é¡µé¢è§£ææµç¨‹
                        break  # è·³å‡ºå¼‚å¸¸å¤„ç†å¾ªç¯ï¼Œä½¿ç”¨è·å¾—çš„é¡µé¢å†…å®¹
                    else:
                        # æ‰‹åŠ¨éªŒè¯å¤±è´¥ï¼Œç»§ç»­åŸæœ‰é€»è¾‘
                        pass
                else:
                    # æˆåŠŸè¯·æ±‚ï¼Œé‡ç½®429è·Ÿè¸ª
                    self._reset_429_tracking()
                
                attempt += 1
                
                # å¦‚æœæ˜¯ç½‘ç»œé”™è¯¯ä¸”æœªå°è¯•è¿‡æµè§ˆå™¨ï¼Œåˆ‡æ¢åˆ°æµè§ˆå™¨æ¨¡å¼å°è¯•
                if not browser_attempt and self.use_browser_fallback:
                    logger.info("ç½‘ç»œè¯·æ±‚å¤±è´¥ï¼Œåˆ‡æ¢åˆ°æµè§ˆå™¨æ–¹å¼å°è¯•...")
                    browser_attempt = True
                    continue
                
                if attempt >= self.max_captcha_retries:
                    logger.error(f"ç½‘ç»œè¯·æ±‚å¤±è´¥æ¬¡æ•°è¿‡å¤šï¼Œæ”¾å¼ƒçˆ¬å–: {cited_by_url}")
                    return []
                    
                # å¯¹äºé429é”™è¯¯ï¼Œä½¿ç”¨æ ‡å‡†å»¶è¿Ÿ
                if "429" not in error_msg and "Too Many Requests" not in error_msg:
                    time.sleep(random.uniform(3, 7) * (attempt + 1))
                browser_attempt = False  # é‡ç½®æµè§ˆå™¨å°è¯•çŠ¶æ€
                continue
                
            except Exception as e:
                logger.error(f"è§£æé¡µé¢æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯ ({cited_by_url}): {e} (å°è¯• {attempt + 1}/{self.max_captcha_retries})")
                # ä¿å­˜è°ƒè¯• HTML for unexpected errors during parsing
                if 'response' in locals() and hasattr(response, 'text'):
                    debug_file = f"debug_parse_error_{int(time.time())}.html"
                    try:
                        with open(debug_file, 'w', encoding='utf-8') as f:
                            f.write(response.text)
                        logger.warning(f"æœªçŸ¥è§£æé”™è¯¯ï¼Œå·²ä¿å­˜è°ƒè¯•é¡µé¢åˆ°: {debug_file}")
                    except Exception as e_debug:
                        logger.error(f"ä¿å­˜è°ƒè¯•é¡µé¢å¤±è´¥: {e_debug}")
                
                attempt += 1
                if attempt >= self.max_captcha_retries:
                    logger.error(f"æœªçŸ¥é”™è¯¯æ¬¡æ•°è¿‡å¤šï¼Œæ”¾å¼ƒçˆ¬å–: {cited_by_url}")
                    return []
                time.sleep(random.uniform(3, 7) * (attempt + 1))
                browser_attempt = False  # é‡ç½®æµè§ˆå™¨å°è¯•çŠ¶æ€
                continue
        
        logger.error(f"æ‰€æœ‰å°è¯•å‡å¤±è´¥: {cited_by_url}")
        return []

    def _get_paper_from_scholar_url(self, scholar_url: str) -> Optional[Paper]:
        """ä»Scholaræœç´¢URLè·å–åŸå§‹è®ºæ–‡ä¿¡æ¯"""
        attempt = 0
        browser_attempt = False  # æ ‡è®°æ˜¯å¦å·²å°è¯•ä½¿ç”¨æµè§ˆå™¨
        
        while attempt < self.max_captcha_retries:
            try:
                logger.info(f"è·å–åŸå§‹è®ºæ–‡ä¿¡æ¯: {scholar_url} (å°è¯• {attempt + 1}/{self.max_captcha_retries})")
                
                # ä½¿ç”¨æµè§ˆå™¨fallbackå°è¯•ç»•è¿‡CAPTCHA
                if browser_attempt and self.use_browser_fallback:
                    logger.info("å°è¯•ä½¿ç”¨æµè§ˆå™¨æ–¹å¼ç»•è¿‡CAPTCHA...")
                    html_content = self._fetch_with_browser(scholar_url)
                    if html_content:
                        soup = BeautifulSoup(html_content, 'html.parser')
                    else:
                        logger.warning("æµè§ˆå™¨è·å–å†…å®¹å¤±è´¥ï¼Œè¿”å›None")
                        return None
                else:
                    # å¸¸è§„è¯·æ±‚æ–¹æ³•
                    self._adaptive_delay()
                    
                    if self.request_count % 5 == 0:
                        self._update_headers()
                    
                    response = self.session.get(scholar_url, timeout=20)
                    response.raise_for_status()
                    
                    soup = BeautifulSoup(response.content, 'html.parser')
                
                # æ£€æµ‹CAPTCHAæˆ–å°ç¦
                if self._is_captcha_page(soup):
                    logger.warning(f"CAPTCHA æ£€æµ‹äºè·å–åŸå§‹è®ºæ–‡: {scholar_url} (å°è¯• {attempt + 1})")
                    
                    # æ‰§è¡ŒCAPTCHAå¤„ç†ç­–ç•¥
                    logger.info("ğŸ”„ æ‰§è¡ŒCAPTCHAå¤„ç†ç­–ç•¥...")
                    
                    # 1. æ›´æ–°è¯·æ±‚å¤´å’Œå¢åŠ éšæœºæ€§
                    self._update_headers()
                    logger.info("   âœ“ å·²æ›´æ–°è¯·æ±‚å¤´")
                    
                    # 2. ä½¿ç”¨æ¸è¿›å¼å»¶è¿Ÿç­–ç•¥
                    retry_delay = 2 + attempt * 2 + random.uniform(1, 3)
                    logger.info(f"   âœ“ æ‰§è¡Œæ¸è¿›å¼å»¶è¿Ÿé‡è¯•: {retry_delay:.1f} ç§’")
                    time.sleep(retry_delay)
                    
                    # 3. å¦‚æœå¯ç”¨äº†429è·³è¿‡æ¨¡å¼ï¼Œä¸ä½¿ç”¨æµè§ˆå™¨å¤„ç†ä½†ç»§ç»­å°è¯•è‡ªåŠ¨ç­–ç•¥
                    if self.skip_429_errors:
                        # æ·»åŠ é¢å¤–éšæœºå»¶è¿Ÿä»¥å¢åŠ ä¸‹æ¬¡æˆåŠŸæ¦‚ç‡
                        if attempt > 0:
                            extra_delay = random.uniform(3, 8)
                            logger.info(f"   âœ“ æ‰§è¡Œé¢å¤–å»¶è¿Ÿ: {extra_delay:.1f} ç§’")
                            time.sleep(extra_delay)
                            
                        logger.info("â­ï¸  è·³è¿‡æ¨¡å¼å·²å¯ç”¨ï¼Œè·³è¿‡æµè§ˆå™¨CAPTCHAå¤„ç†")
                        logger.info("   âœ“ å·²æ‰§è¡Œæ‰€æœ‰è‡ªåŠ¨åŒ–ç­–ç•¥ï¼Œç»§ç»­å°è¯•")
                        attempt += 1
                        continue  # ç»§ç»­ä¸‹ä¸€æ¬¡å°è¯•
                    
                    # 4. é»˜è®¤æ¨¡å¼ï¼šå¦‚æœè¿˜æ²¡å°è¯•è¿‡æµè§ˆå™¨æ–¹æ³•ï¼Œä¸”é…ç½®å…è®¸ï¼Œåˆ™å°è¯•æµè§ˆå™¨æ–¹æ³•
                    if not browser_attempt and self.use_browser_fallback:
                        logger.info("æ£€æµ‹åˆ°CAPTCHAï¼Œåˆ‡æ¢åˆ°æµè§ˆå™¨æ–¹å¼å°è¯•...")
                        browser_attempt = True
                        # ä¸å¢åŠ å°è¯•æ¬¡æ•°ï¼Œç›´æ¥è¿›å…¥ä¸‹ä¸€å¾ªç¯ç”¨æµè§ˆå™¨è®¿é—®
                        continue
                    
                    # å·²ç»å°è¯•è¿‡æµè§ˆå™¨æˆ–ä¸å…è®¸ä½¿ç”¨æµè§ˆå™¨ï¼Œåˆ™å¤„ç†CAPTCHA
                    self._handle_captcha_or_block(scholar_url, response.text if 'response' in locals() else "", attempt)
                    attempt += 1
                    
                    if attempt >= self.max_captcha_retries:
                        logger.error(f"CAPTCHA éªŒè¯å¤±è´¥æ¬¡æ•°è¿‡å¤šï¼Œæ”¾å¼ƒè·å–åŸå§‹è®ºæ–‡: {scholar_url}")
                        return None
                    
                    logger.info(f"CAPTCHA åç­‰å¾…åé‡è¯• ({attempt + 1}/{self.max_captcha_retries}): {scholar_url}")
                    browser_attempt = False  # é‡ç½®æµè§ˆå™¨å°è¯•çŠ¶æ€ï¼Œå†æ¬¡ä»å¸¸è§„è¯·æ±‚å¼€å§‹
                    continue
                
                # æŸ¥æ‰¾ç¬¬ä¸€ä¸ªæœç´¢ç»“æœ
                first_result = soup.find('div', class_='gs_r')
                if not first_result:
                    # å°è¯•å…¶ä»–å¯èƒ½çš„é€‰æ‹©å™¨
                    first_result = soup.find('div', class_='gs_ri') or soup.find('div', {'data-lid': True})
                
                if first_result:
                    return self._parse_paper_info(first_result)
                else:
                    logger.warning(f"æœªæ‰¾åˆ°æœç´¢ç»“æœ for {scholar_url}")
                    # å¦‚æœæ˜¯æµè§ˆå™¨æ–¹å¼è·å–çš„ç»“æœï¼Œä¿å­˜é¡µé¢è¿›è¡Œè°ƒè¯•
                    if browser_attempt:
                        debug_file = f"debug_browser_no_results_{int(time.time())}.html"
                        with open(debug_file, 'w', encoding='utf-8') as f:
                            f.write(soup.prettify())
                        logger.warning(f"æµè§ˆå™¨è·å–é¡µé¢æ— ç»“æœï¼Œå·²ä¿å­˜è°ƒè¯•é¡µé¢åˆ°: {debug_file}")
                    
                    # å¦‚æœé¡µé¢åŠ è½½æˆåŠŸä½†æ²¡æœ‰ç»“æœï¼Œå¯èƒ½æ˜¯çœŸçš„æ‰¾ä¸åˆ°
                    return None
            
            except requests.exceptions.RequestException as e:
                error_msg = str(e)
                logger.error(f"ç½‘ç»œè¯·æ±‚å¤±è´¥è·å–åŸå§‹è®ºæ–‡ ({scholar_url}): {error_msg} (å°è¯• {attempt + 1}/{self.max_captcha_retries})")
                
                # ç‰¹æ®Šå¤„ç†429é”™è¯¯ - Too Many Requests
                if "429" in error_msg or "Too Many Requests" in error_msg:
                    result = self._handle_429_error(scholar_url)
                    if result:  # å¦‚æœæ‰‹åŠ¨éªŒè¯æˆåŠŸï¼Œä½¿ç”¨è¿”å›çš„é¡µé¢å†…å®¹
                        soup = BeautifulSoup(result, 'html.parser')
                        # ç»§ç»­æ­£å¸¸çš„é¡µé¢è§£ææµç¨‹
                        break  # è·³å‡ºå¼‚å¸¸å¤„ç†å¾ªç¯ï¼Œä½¿ç”¨è·å¾—çš„é¡µé¢å†…å®¹
                    else:
                        # æ‰‹åŠ¨éªŒè¯å¤±è´¥ï¼Œç»§ç»­åŸæœ‰é€»è¾‘
                        pass
                else:
                    # æˆåŠŸè¯·æ±‚ï¼Œé‡ç½®429è·Ÿè¸ª
                    self._reset_429_tracking()
                
                attempt += 1
                
                # å¦‚æœæ˜¯ç½‘ç»œé”™è¯¯ä¸”æœªå°è¯•è¿‡æµè§ˆå™¨ï¼Œåˆ‡æ¢åˆ°æµè§ˆå™¨æ¨¡å¼å°è¯•
                if not browser_attempt and self.use_browser_fallback:
                    logger.info("ç½‘ç»œè¯·æ±‚å¤±è´¥ï¼Œåˆ‡æ¢åˆ°æµè§ˆå™¨æ–¹å¼å°è¯•...")
                    browser_attempt = True
                    continue
                
                if attempt >= self.max_captcha_retries:
                    logger.error(f"ç½‘ç»œè¯·æ±‚å¤±è´¥æ¬¡æ•°è¿‡å¤šï¼Œæ”¾å¼ƒè·å–åŸå§‹è®ºæ–‡: {scholar_url}")
                    return None
                
                # å¯¹äºé429é”™è¯¯ï¼Œä½¿ç”¨æ ‡å‡†å»¶è¿Ÿ
                if "429" not in error_msg and "Too Many Requests" not in error_msg:
                    time.sleep(random.uniform(3, 7) * (attempt + 1))
                browser_attempt = False  # é‡ç½®æµè§ˆå™¨å°è¯•çŠ¶æ€
                continue
                
            except Exception as e:
                logger.error(f"è·å–åŸå§‹è®ºæ–‡ä¿¡æ¯æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯ ({scholar_url}): {e} (å°è¯• {attempt + 1}/{self.max_captcha_retries})")
                if 'response' in locals() and hasattr(response, 'text'):
                    debug_file = f"debug_get_paper_error_{int(time.time())}.html"
                    try:
                        with open(debug_file, 'w', encoding='utf-8') as f:
                            f.write(response.text)
                        logger.warning(f"æœªçŸ¥é”™è¯¯è·å–åŸå§‹è®ºæ–‡ï¼Œå·²ä¿å­˜è°ƒè¯•é¡µé¢åˆ°: {debug_file}")
                    except Exception as e_debug:
                        logger.error(f"ä¿å­˜è°ƒè¯•é¡µé¢å¤±è´¥: {e_debug}")
                
                attempt += 1
                if attempt >= self.max_captcha_retries:
                    logger.error(f"æœªçŸ¥é”™è¯¯æ¬¡æ•°è¿‡å¤šï¼Œæ”¾å¼ƒè·å–åŸå§‹è®ºæ–‡: {scholar_url}")
                    return None
                
                time.sleep(random.uniform(3, 7) * (attempt + 1))
                browser_attempt = False  # é‡ç½®æµè§ˆå™¨å°è¯•çŠ¶æ€
                continue
        
        logger.error(f"æ‰€æœ‰å°è¯•å‡å¤±è´¥è·å–åŸå§‹è®ºæ–‡: {scholar_url}")
        return None

    def build_citation_tree(self, start_url: str, current_depth: int = 0) -> Optional[CitationNode]:
        """é€’å½’æ„å»ºå¼•ç”¨æ ‘"""
        if current_depth >= self.max_depth:
            return None
        
        logger.info(f"æ„å»ºå¼•ç”¨æ ‘ï¼Œæ·±åº¦: {current_depth}")
        
        # å¦‚æœæ˜¯èµ·å§‹URLï¼Œéœ€è¦å…ˆè·å–åŸå§‹è®ºæ–‡ä¿¡æ¯
        if current_depth == 0:
            if 'cites=' in start_url:
                # è¿™æ˜¯ä¸€ä¸ª"è¢«å¼•ç”¨"é¡µé¢ï¼Œéœ€è¦è·å–åŸå§‹è®ºæ–‡ä¿¡æ¯
                # æ„é€ ä¸€ä¸ªè™šæ‹Ÿçš„æ ¹è®ºæ–‡
                root_paper = Paper(
                    title="Root Paper (ä»å¼•ç”¨é¡µé¢å¼€å§‹)",
                    authors="Unknown",
                    year="",
                    citation_count=0,
                    url="",
                    cited_by_url=start_url
                )
            else:
                # å°è¯•ä»æœç´¢ç»“æœè·å–è®ºæ–‡ä¿¡æ¯
                root_paper = self._get_paper_from_scholar_url(start_url)
                if not root_paper:
                    logger.error("æ— æ³•è·å–èµ·å§‹è®ºæ–‡ä¿¡æ¯")
                    return None
        else:
            # è¿™ç§æƒ…å†µåœ¨é€’å½’è°ƒç”¨ä¸­ä¸åº”è¯¥å‘ç”Ÿ
            return None
        
        # åˆ›å»ºæ ¹èŠ‚ç‚¹
        root_node = CitationNode(paper=root_paper, children=[], depth=current_depth)
        
        # è·å–å¼•ç”¨è¿™ç¯‡è®ºæ–‡çš„æ–‡ç« ï¼ˆå·²åœ¨_fetch_citationsä¸­æŒ‰å¼•ç”¨é‡æ’åºï¼‰
        citing_papers = self._fetch_citations(root_paper.cited_by_url)
        
        # ä¸ºæ¯ä¸ªå¼•ç”¨è®ºæ–‡é€’å½’æ„å»ºå­æ ‘ï¼ˆè®ºæ–‡å·²æŒ‰å¼•ç”¨é‡é™åºæ’åˆ—ï¼‰
        for citing_paper in citing_papers:
            if citing_paper.cited_by_url and current_depth + 1 < self.max_depth:
                child_node = self._build_citation_subtree(citing_paper, current_depth + 1)
                if child_node:
                    root_node.children.append(child_node)
            else:
                # å¶å­èŠ‚ç‚¹
                leaf_node = CitationNode(paper=citing_paper, children=[], depth=current_depth + 1)
                root_node.children.append(leaf_node)
        
        return root_node
    
    def _build_citation_subtree(self, paper: Paper, depth: int) -> Optional[CitationNode]:
        """é€’å½’æ„å»ºå¼•ç”¨å­æ ‘"""
        if depth >= self.max_depth:
            return None
        
        node = CitationNode(paper=paper, children=[], depth=depth)
        
        # è·å–å¼•ç”¨è¿™ç¯‡è®ºæ–‡çš„æ–‡ç« ï¼ˆå·²åœ¨_fetch_citationsä¸­æŒ‰å¼•ç”¨é‡æ’åºï¼‰
        citing_papers = self._fetch_citations(paper.cited_by_url)
        
        # ä¸ºæ¯ä¸ªå¼•ç”¨è®ºæ–‡é€’å½’æ„å»ºå­æ ‘ï¼ˆè®ºæ–‡å·²æŒ‰å¼•ç”¨é‡é™åºæ’åˆ—ï¼‰
        for citing_paper in citing_papers:
            if citing_paper.cited_by_url and depth + 1 < self.max_depth:
                child_node = self._build_citation_subtree(citing_paper, depth + 1)
                if child_node:
                    node.children.append(child_node)
            else:
                # å¶å­èŠ‚ç‚¹
                leaf_node = CitationNode(paper=citing_paper, children=[], depth=depth + 1)
                node.children.append(leaf_node)
        
        return node

    def _update_headers(self):
        """æ›´æ–°è¯·æ±‚å¤´ï¼Œæ¨¡æ‹ŸçœŸå®æµè§ˆå™¨"""
        user_agent = random.choice(self.user_agents)
        self.session.headers.update({
            'User-Agent': user_agent,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0'
        })
        
    def _rotate_proxy(self):
        """è½®æ¢ä»£ç†æœåŠ¡å™¨"""
        if not self.proxy_list:
            return
            
        self.proxy_index = (self.proxy_index + 1) % len(self.proxy_list)
        self.current_proxy = self.proxy_list[self.proxy_index]
        logger.info(f"è½®æ¢ä»£ç†åˆ°: {self.current_proxy}")
        
        # æ›´æ–°sessionä»£ç†
        if self.current_proxy:
            if self.current_proxy.startswith('http'):
                self.session.proxies = {
                    'http': self.current_proxy,
                    'https': self.current_proxy
                }
            else:
                self.session.proxies = {
                    'http': f'socks5://{self.current_proxy}',
                    'https': f'socks5://{self.current_proxy}'
                }
                
        # æ›´æ–°æµè§ˆå™¨ä»£ç†ï¼ˆå¦‚æœæµè§ˆå™¨å·²åˆå§‹åŒ–ï¼‰
        if self.browser:
            try:
                logger.info("æ­£åœ¨å…³é—­æµè§ˆå™¨ä»¥åº”ç”¨æ–°ä»£ç†...")
                self.browser.quit()
                self.browser = None
                logger.info("æµè§ˆå™¨å·²å…³é—­ï¼Œå°†åœ¨ä¸‹æ¬¡è¯·æ±‚æ—¶é‡æ–°åˆå§‹åŒ–")
            except Exception as e:
                logger.error(f"å…³é—­æµè§ˆå™¨æ—¶å‡ºé”™: {e}")
    
    def _is_captcha_page(self, soup: BeautifulSoup) -> bool:
        """æ£€æµ‹æ˜¯å¦é‡åˆ°äº†CAPTCHAé¡µé¢"""
        captcha_indicators = [
            'please show you\'re not a robot',
            'captcha',
            'recaptcha',
            'robot',
            'verify you are human',
            'unusual traffic'
        ]
        
        page_text = soup.get_text().lower()
        for indicator in captcha_indicators:
            if indicator in page_text:
                return True
        
        # æ£€æŸ¥æ˜¯å¦æœ‰reCAPTCHAç›¸å…³çš„å…ƒç´ 
        if soup.find('div', {'class': 'g-recaptcha'}):
            return True
        if soup.find('iframe', src=lambda x: x and 'recaptcha' in x):
            return True
            
        # Additional indicators for CAPTCHA
        additional_indicators = [
            'unusual traffic from your network',
            'automated requests from your computer',
            'network is sending automated queries'
        ]
        for indicator in additional_indicators:
            if indicator in page_text:
                return True
                
        return False
    
    def _fetch_with_browser(self, url: str) -> Optional[str]:
        """ä½¿ç”¨æµè§ˆå™¨è·å–é¡µé¢å†…å®¹ï¼Œå¯å¤„ç†CAPTCHA"""
        if not BROWSER_AVAILABLE:
            logger.error("æµè§ˆå™¨æ¨¡å—ä¸å¯ç”¨ï¼Œæ— æ³•ä½¿ç”¨æµè§ˆå™¨æ–¹å¼")
            return None
        
        try:
            # åˆå§‹åŒ–æµè§ˆå™¨ï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
            if not self.browser:
                self._init_browser()
            
            if not self.browser:
                logger.error("æµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥")
                return None
            
            logger.info(f"ä½¿ç”¨æµè§ˆå™¨è®¿é—®: {url}")
            self.browser.get(url)
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            time.sleep(3)
            
            # æ£€æŸ¥æ˜¯å¦é‡åˆ°CAPTCHAæˆ–429é”™è¯¯
            page_source = self.browser.page_source
            soup = BeautifulSoup(page_source, 'html.parser')
            
            # æ£€æŸ¥429é”™è¯¯æˆ–CAPTCHA
            page_text = soup.get_text().lower()
            if ('429' in page_text or 'too many requests' in page_text or 
                'unusual traffic' in page_text or 'sorry' in soup.title.string.lower() if soup.title else False):
                logger.warning("æ£€æµ‹åˆ°429é”™è¯¯é¡µé¢")
                if self.skip_429_errors:
                    # åœ¨è·³è¿‡æ¨¡å¼ä¸‹ï¼Œä»å°è¯•ä¸€äº›åŸºæœ¬çš„è‡ªåŠ¨åŒ–ç­–ç•¥
                    logger.info("â­ï¸  æ™ºèƒ½è·³è¿‡æ¨¡å¼å·²å¯ç”¨ï¼Œæ‰§è¡Œè‡ªåŠ¨åŒ–ç­–ç•¥...")
                    
                    # å°è¯•åˆ‡æ¢User-Agent
                    self._update_headers()
                    logger.info("   âœ“ å·²æ›´æ–°User-Agent")
                    
                    # çŸ­æš‚å»¶è¿Ÿåè¿”å›å½“å‰é¡µé¢å†…å®¹ä½œä¸ºæœ€ä½³å°è¯•
                    retry_delay = random.uniform(2, 5)
                    logger.info(f"   âœ“ æ‰§è¡Œå»¶è¿Ÿ: {retry_delay:.1f} ç§’")
                    time.sleep(retry_delay)
                    
                    logger.info("   âœ“ å·²æ‰§è¡Œæ‰€æœ‰è‡ªåŠ¨åŒ–ç­–ç•¥ï¼Œè¿”å›å½“å‰é¡µé¢å†…å®¹")
                    return page_source  # è¿”å›å½“å‰å†…å®¹è€Œä¸æ˜¯None
                    
                logger.warning("åˆ‡æ¢åˆ°æ‰‹åŠ¨å¤„ç†æ¨¡å¼")
                return self._handle_manual_captcha(url)
            
            if self._is_captcha_page(soup):
                logger.warning("æ£€æµ‹åˆ°CAPTCHAé¡µé¢")
                if self.skip_429_errors:
                    # åœ¨è·³è¿‡æ¨¡å¼ä¸‹ï¼Œä»å°è¯•ä¸€äº›åŸºæœ¬çš„è‡ªåŠ¨åŒ–ç­–ç•¥
                    logger.info("â­ï¸  æ™ºèƒ½è·³è¿‡æ¨¡å¼å·²å¯ç”¨ï¼Œæ‰§è¡Œè‡ªåŠ¨åŒ–ç­–ç•¥...")
                    
                    # å°è¯•åˆ‡æ¢User-Agent
                    self._update_headers()
                    logger.info("   âœ“ å·²æ›´æ–°User-Agent")
                    
                    # çŸ­æš‚å»¶è¿Ÿåè¿”å›å½“å‰é¡µé¢å†…å®¹ä½œä¸ºæœ€ä½³å°è¯•
                    retry_delay = random.uniform(2, 5)
                    logger.info(f"   âœ“ æ‰§è¡Œå»¶è¿Ÿ: {retry_delay:.1f} ç§’")
                    time.sleep(retry_delay)
                    
                    logger.info("   âœ“ å·²æ‰§è¡Œæ‰€æœ‰è‡ªåŠ¨åŒ–ç­–ç•¥ï¼Œè¿”å›å½“å‰é¡µé¢å†…å®¹")
                    return page_source  # è¿”å›å½“å‰å†…å®¹è€Œä¸æ˜¯None
                
                logger.warning("éœ€è¦äººå·¥å¤„ç†")
                return self._handle_manual_captcha(url)
            
            return page_source
            
        except Exception as e:
            logger.error(f"æµè§ˆå™¨è®¿é—®å¤±è´¥: {e}")
            return None
    
    def _init_browser(self):
        """åˆå§‹åŒ–æ— å¤´æµè§ˆå™¨"""
        try:
            if not BROWSER_AVAILABLE:
                logger.error("æµè§ˆå™¨ä¾èµ–ä¸å¯ç”¨")
                return
                
            options = uc.ChromeOptions()
            
            # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦ä½¿ç”¨æ— å¤´æ¨¡å¼
            if self.use_headless_browser:
                options.add_argument('--headless')
            
            options.add_argument('--no-sandbox')
            options.add_argument('--disable-dev-shm-usage')
            options.add_argument('--disable-blink-features=AutomationControlled')
            
            # å…¼å®¹æ€§ä¿®å¤ï¼šåªåœ¨æ”¯æŒçš„Chromeç‰ˆæœ¬ä¸­ä½¿ç”¨excludeSwitches
            try:
                options.add_experimental_option("excludeSwitches", ["enable-automation"])
                options.add_experimental_option('useAutomationExtension', False)
            except Exception as ex_e:
                logger.warning(f"è·³è¿‡excludeSwitchesé€‰é¡¹ (å…¼å®¹æ€§é—®é¢˜): {ex_e}")
                # ä½¿ç”¨æ›¿ä»£æ–¹æ³•
                options.add_argument('--disable-automation')
                options.add_argument('--disable-extensions')
            
            # è®¾ç½®ä»£ç†ï¼ˆå¦‚æœæœ‰ï¼‰
            if self.current_proxy:
                if self.current_proxy.startswith('http'):
                    options.add_argument(f'--proxy-server={self.current_proxy}')
                else:
                    options.add_argument(f'--proxy-server=socks5://{self.current_proxy}')
            
            # å°è¯•ä½¿ç”¨å…¼å®¹çš„Chromeé©±åŠ¨åˆå§‹åŒ–
            try:
                self.browser = uc.Chrome(options=options, version_main=None)
            except Exception as chrome_e:
                logger.warning(f"æ ‡å‡†Chromeåˆå§‹åŒ–å¤±è´¥ï¼Œå°è¯•ç®€åŒ–é…ç½®: {chrome_e}")
                # ç®€åŒ–é€‰é¡¹é‡è¯•
                simple_options = uc.ChromeOptions()
                if self.use_headless_browser:
                    simple_options.add_argument('--headless')
                simple_options.add_argument('--no-sandbox')
                simple_options.add_argument('--disable-dev-shm-usage')
                self.browser = uc.Chrome(options=simple_options)
            
            # åªåœ¨æµè§ˆå™¨æˆåŠŸåˆå§‹åŒ–åæ‰§è¡Œè„šæœ¬
            if self.browser:
                try:
                    self.browser.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
                except Exception as script_e:
                    logger.warning(f"æ‰§è¡Œåæ£€æµ‹è„šæœ¬å¤±è´¥: {script_e}")
            
            logger.info("æµè§ˆå™¨åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"æµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.browser = None
    
    def _handle_manual_captcha(self, url: str) -> Optional[str]:
        """å¤„ç†éœ€è¦äººå·¥è§£å†³çš„CAPTCHA"""
        logger.info("=" * 60)
        logger.info("ğŸ¤– æ£€æµ‹åˆ°CAPTCHAæˆ–429é”™è¯¯ï¼Œéœ€è¦äººå·¥å¤„ç†")
        logger.info("=" * 60)
        logger.info(f"è¯·åœ¨æµè§ˆå™¨ä¸­æ‰‹åŠ¨å®ŒæˆCAPTCHAéªŒè¯")
        logger.info(f"é¡µé¢URL: {url}")
        
        # å¼ºåˆ¶åˆ‡æ¢åˆ°æœ‰å¤´æ¨¡å¼
        original_headless = self.use_headless_browser
        self.use_headless_browser = False
        
        try:
            # å…³é—­ç°æœ‰æµè§ˆå™¨ï¼ˆå¦‚æœæœ‰ï¼‰
            if self.browser:
                try:
                    self.browser.quit()
                except:
                    pass
                self.browser = None
            
            # é‡æ–°åˆå§‹åŒ–ä¸ºæœ‰å¤´æ¨¡å¼
            self._init_browser()
            
            if not self.browser:
                logger.error("æ— æ³•åˆå§‹åŒ–æµè§ˆå™¨è¿›è¡Œæ‰‹åŠ¨CAPTCHAå¤„ç†")
                return None
            
            # å¯¼èˆªåˆ°é¡µé¢
            logger.info("ğŸŒ æ­£åœ¨æ‰“å¼€æµè§ˆå™¨çª—å£...")
            self.browser.get(url)
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            time.sleep(3)
            
            logger.info("ğŸ¯ æµè§ˆå™¨çª—å£å·²æ‰“å¼€ï¼")
            logger.info("è¯·åœ¨æµè§ˆå™¨ä¸­ï¼š")
            logger.info("1. å®Œæˆä»»ä½•CAPTCHAéªŒè¯")
            logger.info("2. ç­‰å¾…é¡µé¢æ­£å¸¸åŠ è½½")
            logger.info("3. å¦‚æœé‡åˆ°403/429é”™è¯¯é¡µé¢ï¼Œè¯·åˆ·æ–°é¡µé¢ç›´åˆ°æ­£å¸¸")
            logger.info("4. âš ï¸  è¯·ä¸è¦å…³é—­æµè§ˆå™¨çª—å£ï¼")
            logger.info("5. å®Œæˆåï¼Œè¯·å›åˆ°ç»ˆç«¯æŒ‰å›è½¦é”®ç»§ç»­...")
            logger.info("=" * 60)
            
            input("\nâ³ è¯·å®Œæˆæµè§ˆå™¨ä¸­çš„éªŒè¯ï¼Œç„¶åæŒ‰å›è½¦é”®ç»§ç»­...")
            
            # è·å–å½“å‰é¡µé¢å†…å®¹
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    logger.info(f"ğŸ”„ æ­£åœ¨è·å–é¡µé¢å†…å®¹ (å°è¯• {attempt + 1}/{max_retries})...")
                    
                    # æ£€æŸ¥æµè§ˆå™¨çª—å£æ˜¯å¦è¿˜å­˜åœ¨
                    try:
                        window_handles = self.browser.window_handles
                        if not window_handles:
                            logger.warning("âš ï¸  æµè§ˆå™¨çª—å£å·²å…³é—­")
                            retry_choice = input("æµè§ˆå™¨çª—å£å·²å…³é—­ï¼Œæ˜¯å¦é‡æ–°æ‰“å¼€ï¼Ÿ(y/n): ").lower().strip()
                            if retry_choice == 'y':
                                return self._handle_manual_captcha(url)
                            else:
                                return None
                    except Exception as e:
                        logger.warning(f"âš ï¸  æ— æ³•æ£€æŸ¥æµè§ˆå™¨çŠ¶æ€: {e}")
                        retry_choice = input("æµè§ˆå™¨è¿æ¥å¼‚å¸¸ï¼Œæ˜¯å¦é‡æ–°æ‰“å¼€ï¼Ÿ(y/n): ").lower().strip()
                        if retry_choice == 'y':
                            return self._handle_manual_captcha(url)
                        else:
                            return None
                    
                    # ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½
                    time.sleep(3)
                    
                    # è·å–å½“å‰URL
                    current_url = self.browser.current_url
                    logger.info(f"ğŸ“ å½“å‰é¡µé¢URL: {current_url}")
                    
                    # æ£€æŸ¥é¡µé¢æ˜¯å¦è¿˜åœ¨åŠ è½½
                    page_state = self.browser.execute_script("return document.readyState")
                    logger.info(f"ğŸ“„ é¡µé¢çŠ¶æ€: {page_state}")
                    
                    if page_state != "complete":
                        logger.info("â³ é¡µé¢ä»åœ¨åŠ è½½ï¼Œç­‰å¾…...")
                        time.sleep(5)
                    
                    # è·å–é¡µé¢æºç 
                    page_source = self.browser.page_source
                    
                    # æ£€æŸ¥é¡µé¢æºç é•¿åº¦
                    page_length = len(page_source) if page_source else 0
                    logger.info(f"ğŸ“ é¡µé¢å†…å®¹é•¿åº¦: {page_length} å­—ç¬¦")
                    
                    if not page_source or page_length < 100:
                        logger.warning(f"âš ï¸  é¡µé¢å†…å®¹å¤ªçŸ­æˆ–ä¸ºç©º (é•¿åº¦: {page_length})")
                        if attempt < max_retries - 1:
                            logger.info("ğŸ”„ å°†é‡æ–°å°è¯•è·å–...")
                            continue
                        else:
                            retry = input("é¡µé¢å†…å®¹å¼‚å¸¸ï¼Œæ˜¯å¦æ‰‹åŠ¨é‡è¯•ï¼Ÿ(y/n): ").lower().strip()
                            if retry == 'y':
                                return self._handle_manual_captcha(url)
                            else:
                                return None
                    
                    # è§£æé¡µé¢å†…å®¹
                    soup = BeautifulSoup(page_source, 'html.parser')
                    page_text = soup.get_text().lower() if soup else ""
                    
                    # è¾“å‡ºé¡µé¢è°ƒè¯•ä¿¡æ¯
                    page_title = soup.title.string if soup.title else "æ— æ ‡é¢˜"
                    logger.info(f"ğŸ“° é¡µé¢æ ‡é¢˜: {page_title}")
                    
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«Scholarå†…å®¹
                    has_scholar_content = (
                        "scholar" in page_text or 
                        "google scholar" in page_text or
                        "cited by" in page_text or
                        "citations" in page_text or
                        "ç»“æœ" in page_text or  # ä¸­æ–‡ç‰ˆæœ¬
                        "results" in page_text
                    )
                    
                    # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰CAPTCHAæˆ–é”™è¯¯
                    has_captcha = self._is_captcha_page(soup)
                    has_errors = (
                        "sorry" in page_text or 
                        "unusual traffic" in page_text or
                        "too many requests" in page_text or
                        "403" in page_text or
                        "forbidden" in page_text
                    )
                    
                    logger.info(f"ğŸ” é¡µé¢æ£€æŸ¥ç»“æœ:")
                    logger.info(f"   - åŒ…å«Scholarå†…å®¹: {'æ˜¯' if has_scholar_content else 'å¦'}")
                    logger.info(f"   - æ£€æµ‹åˆ°CAPTCHA: {'æ˜¯' if has_captcha else 'å¦'}")
                    logger.info(f"   - æ£€æµ‹åˆ°é”™è¯¯: {'æ˜¯' if has_errors else 'å¦'}")
                    
                    if not has_captcha and not has_errors:
                        if has_scholar_content:
                            logger.info("âœ… éªŒè¯æˆåŠŸï¼é¡µé¢å·²æ­£å¸¸åŠ è½½ï¼ŒåŒ…å«Scholarå†…å®¹")
                            return page_source
                        else:
                            # å³ä½¿æ²¡æœ‰æ˜ç¡®çš„Scholaræ ‡è¯†ï¼Œä¹Ÿå¯èƒ½æ˜¯æ­£å¸¸é¡µé¢
                            logger.warning("âš ï¸  æœªæ˜ç¡®æ£€æµ‹åˆ°Scholarå†…å®¹ï¼Œä½†é¡µé¢ä¼¼ä¹æ­£å¸¸")
                            logger.info("ğŸ’¡ æç¤ºï¼šè¿™å¯èƒ½æ˜¯å› ä¸ºé¡µé¢å†…å®¹æ ¼å¼å˜åŒ–æˆ–è¯­è¨€è®¾ç½®")
                            use_anyway = input("æ˜¯å¦ä»è¦ä½¿ç”¨æ­¤é¡µé¢å†…å®¹ï¼Ÿ(y/n): ").lower().strip()
                            if use_anyway == 'y':
                                logger.info("âœ… ç”¨æˆ·ç¡®è®¤ä½¿ç”¨é¡µé¢å†…å®¹")
                                return page_source
                            else:
                                retry = input("æ˜¯å¦é‡æ–°å°è¯•éªŒè¯ï¼Ÿ(y/n): ").lower().strip()
                                if retry == 'y':
                                    return self._handle_manual_captcha(url)
                                else:
                                    return None
                    else:
                        logger.warning("âš ï¸  é¡µé¢ä¼¼ä¹ä»æœ‰é—®é¢˜")
                        if has_captcha:
                            logger.info("ğŸ” æ£€æµ‹åˆ°CAPTCHAï¼Œè¯·ç¡®ä¿å·²å®ŒæˆéªŒè¯")
                        if has_errors:
                            logger.info("ğŸ” æ£€æµ‹åˆ°é”™è¯¯é¡µé¢ï¼Œè¯·åˆ·æ–°é¡µé¢æˆ–ç­‰å¾…")
                        
                        retry = input("æ˜¯å¦é‡è¯•ï¼Ÿ(y/n): ").lower().strip()
                        if retry == 'y':
                            return self._handle_manual_captcha(url)
                        else:
                            return None
                    
                except Exception as e:
                    logger.error(f"è·å–é¡µé¢å†…å®¹æ—¶å‡ºé”™: {e}")
                    if attempt < max_retries - 1:
                        logger.info("ğŸ”„ å°†é‡æ–°å°è¯•...")
                        time.sleep(2)
                        continue
                    else:
                        retry = input("è·å–é¡µé¢å†…å®¹å¤±è´¥ï¼Œæ˜¯å¦é‡æ–°å°è¯•ï¼Ÿ(y/n): ").lower().strip()
                        if retry == 'y':
                            return self._handle_manual_captcha(url)
                        else:
                            return None
            
            logger.error("æ— æ³•è·å–æœ‰æ•ˆçš„é¡µé¢å†…å®¹")
            return None
            
        finally:
            # æ¢å¤åŸå§‹çš„æ— å¤´æ¨¡å¼è®¾ç½®
            self.use_headless_browser = original_headless
    
    def _handle_captcha_or_block(self, url: str, response_text: str, attempt: int):
        """å¤„ç†CAPTCHAæˆ–å°ç¦çš„é€šç”¨æ–¹æ³•"""
        logger.warning(f"é‡åˆ°CAPTCHAæˆ–å°ç¦ (å°è¯• {attempt + 1})")
        
        # è½®æ¢ä»£ç†ï¼ˆå¦‚æœæœ‰ï¼‰
        if self.proxy_list:
            self._rotate_proxy()
            logger.info("å·²è½®æ¢ä»£ç†")
        
        # æ›´æ–°è¯·æ±‚å¤´
        self._update_headers()
        logger.info("å·²æ›´æ–°è¯·æ±‚å¤´")
        
        # æ¸è¿›å¼å»¶è¿Ÿ
        delay = 5 + attempt * 3 + random.uniform(1, 5)
        logger.info(f"ç­‰å¾… {delay:.1f} ç§’åé‡è¯•...")
        time.sleep(delay)
    
    def _adaptive_delay(self):
        """è‡ªé€‚åº”å»¶è¿Ÿç­–ç•¥"""
        self.request_count += 1
        
        # åŸºç¡€å»¶è¿Ÿ
        base_delay = random.uniform(*self.delay_range)
        
        # æ ¹æ®è¯·æ±‚é¢‘ç‡è°ƒæ•´å»¶è¿Ÿ
        if self.request_count % 10 == 0:
            base_delay *= 1.5  # æ¯10ä¸ªè¯·æ±‚å¢åŠ 50%å»¶è¿Ÿ
        
        # å¦‚æœæœ€è¿‘é‡åˆ°è¿‡429é”™è¯¯ï¼Œå¢åŠ å»¶è¿Ÿ
        if self.last_429_time:
            time_since_429 = datetime.now() - self.last_429_time
            if time_since_429 < timedelta(minutes=5):
                base_delay *= 2
        
        time.sleep(base_delay)
    
    def _handle_429_error(self, url: str) -> Optional[str]:
        """å¤„ç†429é”™è¯¯ - Too Many Requests"""
        current_time = datetime.now()
        self.last_429_time = current_time
        self.consecutive_429_count += 1
        
        logger.warning(f"é‡åˆ°429é”™è¯¯ (è¿ç»­ç¬¬{self.consecutive_429_count}æ¬¡): {url}")
        
        if self.skip_429_errors:
            logger.info("â­ï¸  å¯ç”¨äº†è·³è¿‡429é”™è¯¯æ¨¡å¼ï¼Œæ‰§è¡Œå¿«é€Ÿç­–ç•¥")
            
            # å¿«é€Ÿç­–ç•¥ï¼šçŸ­æš‚å»¶è¿Ÿåç»§ç»­
            retry_delay = 2 + random.uniform(1, 3)
            logger.info(f"   âœ“ æ‰§è¡Œå¿«é€Ÿå»¶è¿Ÿ: {retry_delay:.1f} ç§’")
            time.sleep(retry_delay)
            
            # æ›´æ–°è¯·æ±‚å¤´
            self._update_headers()
            logger.info("   âœ“ å·²æ›´æ–°è¯·æ±‚å¤´")
            
            logger.info("   âœ“ å¿«é€Ÿç­–ç•¥å®Œæˆï¼Œç»§ç»­å°è¯•")
            return None  # è¿”å›Noneï¼Œè®©è°ƒç”¨æ–¹ç»§ç»­å°è¯•
        
        # é»˜è®¤æ¨¡å¼ï¼šå®Œæ•´çš„429å¤„ç†ç­–ç•¥
        logger.info("ğŸ”„ æ‰§è¡Œå®Œæ•´çš„429é”™è¯¯å¤„ç†ç­–ç•¥...")
        
        # 1. è½®æ¢ä»£ç†
        if self.proxy_list:
            self._rotate_proxy()
            logger.info("   âœ“ å·²è½®æ¢ä»£ç†")
        
        # 2. æ›´æ–°è¯·æ±‚å¤´
        self._update_headers()
        logger.info("   âœ“ å·²æ›´æ–°è¯·æ±‚å¤´")
        
        # 3. è®¡ç®—å»¶è¿Ÿæ—¶é—´
        base_delay = 10  # åŸºç¡€å»¶è¿Ÿ10ç§’
        progressive_delay = self.consecutive_429_count * 5  # æ¸è¿›å¼å»¶è¿Ÿ
        random_delay = random.uniform(5, 15)  # éšæœºå»¶è¿Ÿ
        total_delay = base_delay + progressive_delay + random_delay
        
        logger.info(f"   âœ“ æ‰§è¡Œå»¶è¿Ÿç­–ç•¥: {total_delay:.1f} ç§’")
        logger.info(f"     - åŸºç¡€å»¶è¿Ÿ: {base_delay}s")
        logger.info(f"     - æ¸è¿›å»¶è¿Ÿ: {progressive_delay}s (è¿ç»­{self.consecutive_429_count}æ¬¡)")
        logger.info(f"     - éšæœºå»¶è¿Ÿ: {random_delay:.1f}s")
        
        time.sleep(total_delay)
        
        # 4. å¦‚æœè¿ç»­429é”™è¯¯å¤ªå¤šï¼Œå¯ç”¨æ‰‹åŠ¨éªŒè¯
        if self.consecutive_429_count >= 3 and self.use_browser_fallback:
            logger.warning("è¿ç»­429é”™è¯¯è¿‡å¤šï¼Œå¯ç”¨æ‰‹åŠ¨éªŒè¯æ¨¡å¼")
            return self._handle_manual_captcha(url)
        
        logger.info("   âœ“ 429é”™è¯¯å¤„ç†å®Œæˆï¼Œç»§ç»­å°è¯•")
        return None
    
    def _reset_429_tracking(self):
        """é‡ç½®429é”™è¯¯è·Ÿè¸ª"""
        if self.consecutive_429_count > 0:
            logger.info(f"âœ… æˆåŠŸè¯·æ±‚ï¼Œé‡ç½®429é”™è¯¯è®¡æ•° (ä¹‹å‰è¿ç»­{self.consecutive_429_count}æ¬¡)")
        self.consecutive_429_count = 0
        self.last_429_time = None
    
    def close(self):
        """æ¸…ç†èµ„æº"""
        if self.browser:
            try:
                self.browser.quit()
                logger.info("æµè§ˆå™¨å·²å…³é—­")
            except Exception as e:
                logger.error(f"å…³é—­æµè§ˆå™¨æ—¶å‡ºé”™: {e}")
        
        if self.session:
            self.session.close()
            logger.info("ä¼šè¯å·²å…³é—­")

def print_citation_tree(node: CitationNode, indent: int = 0, max_title_length: int = 80):
    """æ‰“å°å¼•ç”¨æ ‘çš„ç¾åŒ–ç‰ˆæœ¬"""
    if not node:
        return
    
    prefix = "  " * indent
    
    # æˆªæ–­è¿‡é•¿çš„æ ‡é¢˜
    title = node.paper.title
    if len(title) > max_title_length:
        title = title[:max_title_length-3] + "..."
    
    # æ ¼å¼åŒ–è¾“å‡º
    citation_info = f"(å¼•ç”¨æ•°: {node.paper.citation_count})" if node.paper.citation_count > 0 else ""
    year_info = f"({node.paper.year})" if node.paper.year else ""
    
    print(f"{prefix}â”œâ”€ {title}")
    if node.paper.authors:
        print(f"{prefix}   ä½œè€…: {node.paper.authors}")
    if year_info or citation_info:
        info_line = " ".join(filter(None, [year_info, citation_info]))
        print(f"{prefix}   {info_line}")
    if node.paper.url:
        print(f"{prefix}   é“¾æ¥: {node.paper.url}")
    print()
    
    # é€’å½’æ‰“å°å­èŠ‚ç‚¹
    for child in node.children:
        print_citation_tree(child, indent + 1, max_title_length)

def save_tree_to_json(node: CitationNode, filename: str):
    """å°†å¼•ç”¨æ ‘ä¿å­˜ä¸ºJSONæ ¼å¼"""
    def node_to_dict(n: CitationNode) -> dict:
        return {
            'paper': {
                'title': n.paper.title,
                'authors': n.paper.authors,
                'year': n.paper.year,
                'citation_count': n.paper.citation_count,
                'url': n.paper.url,
                'cited_by_url': n.paper.cited_by_url,
                'abstract': n.paper.abstract
            },
            'depth': n.depth,
            'children': [node_to_dict(child) for child in n.children]
        }
    
    tree_dict = node_to_dict(node)
    
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(tree_dict, f, ensure_ascii=False, indent=2)
    
    logger.info(f"å¼•ç”¨æ ‘å·²ä¿å­˜åˆ°: {filename}")

def load_tree_from_json(filename: str) -> CitationNode:
    """ä»JSONæ–‡ä»¶åŠ è½½å¼•ç”¨æ ‘"""
    def dict_to_node(data: dict) -> CitationNode:
        paper = Paper(**data['paper'])
        node = CitationNode(paper=paper, children=[], depth=data['depth'])
        node.children = [dict_to_node(child) for child in data['children']]
        return node
    
    with open(filename, 'r', encoding='utf-8') as f:
        tree_dict = json.load(f)
    
    return dict_to_node(tree_dict)

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # ç¤ºä¾‹ä½¿ç”¨
    crawler = GoogleScholarCrawler(
        max_depth=2,
        max_papers_per_level=5,
        delay_range=(2, 4),
        skip_429_errors=True  # å¯ç”¨è·³è¿‡429é”™è¯¯æ¨¡å¼
    )
    
    # ä»å¼•ç”¨é¡µé¢å¼€å§‹çˆ¬å–
    start_url = "https://scholar.google.com/scholar?cites=1234567890&as_sdt=2005&sciodt=0,5&hl=en"
    
    try:
        tree = crawler.build_citation_tree(start_url)
        if tree:
            print_citation_tree(tree)
            save_tree_to_json(tree, "citation_tree.json")
        else:
            print("æœªèƒ½æ„å»ºå¼•ç”¨æ ‘")
    finally:
        crawler.close()
