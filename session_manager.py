#!/usr/bin/env python3
"""
PaperTracer ä¼šè¯ç®¡ç†å·¥å…·
ç”¨äºåˆ†æã€æ¢å¤å’Œç®¡ç†çˆ¬è™«ä¼šè¯
"""

import sys
import os
import json
import argparse
from pathlib import Path
from datetime import datetime, timedelta
from papertracer_config import Config
from logger import get_logger

def setup_session_manager_parser():
    """è®¾ç½®ä¼šè¯ç®¡ç†å™¨å‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description='PaperTracer ä¼šè¯ç®¡ç†å·¥å…·',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
åŠŸèƒ½:
  list              - åˆ—å‡ºæ‰€æœ‰å¯ç”¨ä¼šè¯
  analyze SESSION   - åˆ†ææŒ‡å®šä¼šè¯çš„ç»Ÿè®¡ä¿¡æ¯
  cleanup           - æ¸…ç†è¿‡æœŸæˆ–æ— æ•ˆä¼šè¯
  merge SESSION1 SESSION2 - åˆå¹¶ä¸¤ä¸ªä¼šè¯çš„æ•°æ®
  export SESSION    - å¯¼å‡ºä¼šè¯æ•°æ®ä¸ºå¯è¯»æ ¼å¼

ç¤ºä¾‹ç”¨æ³•:
  python session_manager.py list
  python session_manager.py analyze demo_20240602_123456
  python session_manager.py cleanup --days 30
  python session_manager.py export demo_20240602_123456 --format csv
        """
    )
    
    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')
    
    # list å‘½ä»¤
    list_parser = subparsers.add_parser('list', help='åˆ—å‡ºæ‰€æœ‰ä¼šè¯')
    list_parser.add_argument('--recent', type=int, help='åªæ˜¾ç¤ºæœ€è¿‘Nä¸ªä¼šè¯')
    list_parser.add_argument('--detailed', action='store_true', help='æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯')
    
    # analyze å‘½ä»¤
    analyze_parser = subparsers.add_parser('analyze', help='åˆ†æä¼šè¯')
    analyze_parser.add_argument('session_id', help='ä¼šè¯ID')
    analyze_parser.add_argument('--export-stats', help='å¯¼å‡ºç»Ÿè®¡ä¿¡æ¯åˆ°æ–‡ä»¶')
    
    # cleanup å‘½ä»¤
    cleanup_parser = subparsers.add_parser('cleanup', help='æ¸…ç†ä¼šè¯')
    cleanup_parser.add_argument('--days', type=int, default=30, help='åˆ é™¤Nå¤©å‰çš„ä¼šè¯')
    cleanup_parser.add_argument('--dry-run', action='store_true', help='åªæ˜¾ç¤ºå°†è¢«åˆ é™¤çš„ä¼šè¯ï¼Œä¸å®é™…åˆ é™¤')
    cleanup_parser.add_argument('--force', action='store_true', help='å¼ºåˆ¶åˆ é™¤ï¼Œä¸è¯¢é—®ç¡®è®¤')
    
    # merge å‘½ä»¤
    merge_parser = subparsers.add_parser('merge', help='åˆå¹¶ä¼šè¯')
    merge_parser.add_argument('session1', help='ç¬¬ä¸€ä¸ªä¼šè¯ID')
    merge_parser.add_argument('session2', help='ç¬¬äºŒä¸ªä¼šè¯ID')
    merge_parser.add_argument('--output', help='è¾“å‡ºä¼šè¯åç§°')
    
    # export å‘½ä»¤
    export_parser = subparsers.add_parser('export', help='å¯¼å‡ºä¼šè¯æ•°æ®')
    export_parser.add_argument('session_id', help='ä¼šè¯ID')
    export_parser.add_argument('--format', choices=['json', 'csv', 'txt'], default='txt', help='å¯¼å‡ºæ ¼å¼')
    export_parser.add_argument('--output', help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    
    return parser

class SessionManager:
    """ä¼šè¯ç®¡ç†å™¨ç±»"""
    
    def __init__(self):
        self.logger = get_logger()
        self.output_dir = Path(Config.OUTPUT_DIR)
        
    def get_all_sessions(self):
        """è·å–æ‰€æœ‰ä¼šè¯ç›®å½•"""
        sessions = []
        if not self.output_dir.exists():
            return sessions
            
        for item in self.output_dir.iterdir():
            if item.is_dir() and not item.name.startswith('.'):
                session_info = self._get_session_info(item)
                if session_info:
                    sessions.append(session_info)
        
        return sorted(sessions, key=lambda x: x['created_time'], reverse=True)
    
    def _get_session_info(self, session_dir):
        """è·å–ä¼šè¯åŸºæœ¬ä¿¡æ¯"""
        try:
            session_state_file = session_dir / "session_state.json"
            citation_files = list(session_dir.glob("*citation_tree*.json"))
            
            info = {
                'id': session_dir.name,
                'path': session_dir,
                'created_time': datetime.fromtimestamp(session_dir.stat().st_ctime),
                'modified_time': datetime.fromtimestamp(session_dir.stat().st_mtime),
                'has_session_state': session_state_file.exists(),
                'has_citation_data': len(citation_files) > 0,
                'file_count': len(list(session_dir.iterdir())),
                'size_mb': sum(f.stat().st_size for f in session_dir.rglob('*') if f.is_file()) / (1024 * 1024)
            }
            
            # å¦‚æœæœ‰ä¼šè¯çŠ¶æ€æ–‡ä»¶ï¼Œè¯»å–è¯¦ç»†ä¿¡æ¯
            if info['has_session_state']:
                try:
                    with open(session_state_file, 'r') as f:
                        state = json.load(f)
                    info.update({
                        'request_count': state.get('request_count', 0),
                        'visited_urls': len(state.get('visited_urls', [])),
                        'consecutive_429_count': state.get('consecutive_429_count', 0),
                        'last_429_time': state.get('last_429_time')
                    })
                except:
                    pass
            
            return info
        except:
            return None
    
    def list_sessions(self, recent=None, detailed=False):
        """åˆ—å‡ºä¼šè¯"""
        sessions = self.get_all_sessions()
        
        if recent:
            sessions = sessions[:recent]
        
        if not sessions:
            self.logger.info("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•ä¼šè¯")
            return
        
        self.logger.info(f"æ‰¾åˆ° {len(sessions)} ä¸ªä¼šè¯:")
        self.logger.info("=" * 80)
        
        for session in sessions:
            age = datetime.now() - session['created_time']
            age_str = f"{age.days}å¤©å‰" if age.days > 0 else "ä»Šå¤©"
            
            status = "ğŸŸ¢" if session['has_citation_data'] else "ğŸŸ¡"
            if session['has_session_state']:
                status += " ğŸ“Š"
            
            self.logger.info(f"{status} {session['id']}")
            self.logger.info(f"    åˆ›å»ºæ—¶é—´: {session['created_time'].strftime('%Y-%m-%d %H:%M:%S')} ({age_str})")
            self.logger.info(f"    å¤§å°: {session['size_mb']:.1f} MB, æ–‡ä»¶æ•°: {session['file_count']}")
            
            if detailed and session['has_session_state']:
                self.logger.info(f"    è¯·æ±‚æ•°: {session.get('request_count', 0)}")
                self.logger.info(f"    è®¿é—®URLæ•°: {session.get('visited_urls', 0)}")
                self.logger.info(f"    429é”™è¯¯: {session.get('consecutive_429_count', 0)}")
            
            self.logger.info("")
    
    def analyze_session(self, session_id, export_stats=None):
        """åˆ†ææŒ‡å®šä¼šè¯"""
        session_dir = self.output_dir / session_id
        if not session_dir.exists():
            self.logger.error(f"ä¼šè¯ä¸å­˜åœ¨: {session_id}")
            return False
        
        session_info = self._get_session_info(session_dir)
        if not session_info:
            self.logger.error(f"æ— æ³•è¯»å–ä¼šè¯ä¿¡æ¯: {session_id}")
            return False
        
        self.logger.info(f"ğŸ“Š ä¼šè¯åˆ†æ: {session_id}")
        self.logger.info("=" * 60)
        
        # åŸºæœ¬ä¿¡æ¯
        self.logger.info("åŸºæœ¬ä¿¡æ¯:")
        self.logger.info(f"  åˆ›å»ºæ—¶é—´: {session_info['created_time'].strftime('%Y-%m-%d %H:%M:%S')}")
        self.logger.info(f"  ä¿®æ”¹æ—¶é—´: {session_info['modified_time'].strftime('%Y-%m-%d %H:%M:%S')}")
        self.logger.info(f"  æ–‡ä»¶å¤§å°: {session_info['size_mb']:.1f} MB")
        self.logger.info(f"  æ–‡ä»¶æ•°é‡: {session_info['file_count']}")
        
        # çˆ¬è™«ç»Ÿè®¡
        if session_info['has_session_state']:
            self.logger.info("\nçˆ¬è™«ç»Ÿè®¡:")
            self.logger.info(f"  æ€»è¯·æ±‚æ•°: {session_info.get('request_count', 0)}")
            self.logger.info(f"  è®¿é—®URLæ•°: {session_info.get('visited_urls', 0)}")
            self.logger.info(f"  429é”™è¯¯æ¬¡æ•°: {session_info.get('consecutive_429_count', 0)}")
            
            if session_info.get('last_429_time'):
                self.logger.info(f"  æœ€å429é”™è¯¯: {session_info['last_429_time']}")
        
        # æ–‡ä»¶åˆ†æ
        files = list(session_dir.iterdir())
        file_types = {}
        for file in files:
            if file.is_file():
                ext = file.suffix.lower()
                file_types[ext] = file_types.get(ext, 0) + 1
        
        self.logger.info("\næ–‡ä»¶ç±»å‹åˆ†å¸ƒ:")
        for ext, count in sorted(file_types.items()):
            self.logger.info(f"  {ext or 'æ— æ‰©å±•å'}: {count} ä¸ªæ–‡ä»¶")
        
        # å¼•ç”¨æ•°æ®åˆ†æ
        citation_files = list(session_dir.glob("*citation_tree*.json"))
        if citation_files:
            self.logger.info("\nå¼•ç”¨æ ‘åˆ†æ:")
            for citation_file in citation_files:
                try:
                    with open(citation_file, 'r') as f:
                        tree_data = json.load(f)
                    
                    # ç»Ÿè®¡èŠ‚ç‚¹æ•°é‡
                    node_count = self._count_nodes(tree_data)
                    max_depth = self._get_max_depth(tree_data, 0)
                    
                    self.logger.info(f"  æ–‡ä»¶: {citation_file.name}")
                    self.logger.info(f"    èŠ‚ç‚¹æ€»æ•°: {node_count}")
                    self.logger.info(f"    æœ€å¤§æ·±åº¦: {max_depth}")
                    
                except Exception as e:
                    self.logger.warning(f"  æ— æ³•åˆ†æ {citation_file.name}: {e}")
        
        # å¯¼å‡ºç»Ÿè®¡ä¿¡æ¯
        if export_stats:
            stats = {
                'session_id': session_id,
                'analysis_time': datetime.now().isoformat(),
                'basic_info': {
                    'created_time': session_info['created_time'].isoformat(),
                    'size_mb': session_info['size_mb'],
                    'file_count': session_info['file_count']
                },
                'crawler_stats': {
                    'request_count': session_info.get('request_count', 0),
                    'visited_urls': session_info.get('visited_urls', 0),
                    'consecutive_429_count': session_info.get('consecutive_429_count', 0)
                },
                'file_types': file_types
            }
            
            try:
                with open(export_stats, 'w') as f:
                    json.dump(stats, f, indent=2)
                self.logger.info(f"\nâœ… ç»Ÿè®¡ä¿¡æ¯å·²å¯¼å‡ºåˆ°: {export_stats}")
            except Exception as e:
                self.logger.error(f"å¯¼å‡ºç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
        
        return True
    
    def _count_nodes(self, node):
        """é€’å½’è®¡ç®—èŠ‚ç‚¹æ•°é‡"""
        if not node:
            return 0
        count = 1
        if 'children' in node:
            for child in node['children']:
                count += self._count_nodes(child)
        return count
    
    def _get_max_depth(self, node, current_depth):
        """é€’å½’è®¡ç®—æœ€å¤§æ·±åº¦"""
        if not node or 'children' not in node or not node['children']:
            return current_depth
        
        max_child_depth = current_depth
        for child in node['children']:
            child_depth = self._get_max_depth(child, current_depth + 1)
            max_child_depth = max(max_child_depth, child_depth)
        
        return max_child_depth
    
    def cleanup_sessions(self, days=30, dry_run=False, force=False):
        """æ¸…ç†è¿‡æœŸä¼šè¯"""
        cutoff_date = datetime.now() - timedelta(days=days)
        sessions = self.get_all_sessions()
        
        old_sessions = [s for s in sessions if s['created_time'] < cutoff_date]
        
        if not old_sessions:
            self.logger.info(f"æ²¡æœ‰æ‰¾åˆ° {days} å¤©å‰çš„ä¼šè¯")
            return
        
        self.logger.info(f"æ‰¾åˆ° {len(old_sessions)} ä¸ªè¶…è¿‡ {days} å¤©çš„ä¼šè¯:")
        
        total_size = sum(s['size_mb'] for s in old_sessions)
        for session in old_sessions:
            age = datetime.now() - session['created_time']
            self.logger.info(f"  - {session['id']} ({session['size_mb']:.1f} MB, {age.days} å¤©å‰)")
        
        self.logger.info(f"æ€»å¤§å°: {total_size:.1f} MB")
        
        if dry_run:
            self.logger.info("\nğŸ” è¿™æ˜¯é¢„è§ˆæ¨¡å¼ï¼Œæ²¡æœ‰å®é™…åˆ é™¤ä»»ä½•æ–‡ä»¶")
            return
        
        if not force:
            response = input(f"\nç¡®å®šè¦åˆ é™¤è¿™ {len(old_sessions)} ä¸ªä¼šè¯å—? (y/N): ")
            if response.lower() != 'y':
                self.logger.info("æ“ä½œå·²å–æ¶ˆ")
                return
        
        # æ‰§è¡Œåˆ é™¤
        deleted_count = 0
        for session in old_sessions:
            try:
                import shutil
                shutil.rmtree(session['path'])
                deleted_count += 1
                self.logger.info(f"âœ… å·²åˆ é™¤: {session['id']}")
            except Exception as e:
                self.logger.error(f"âŒ åˆ é™¤å¤±è´¥ {session['id']}: {e}")
        
        self.logger.info(f"\nğŸ§¹ æ¸…ç†å®Œæˆ: åˆ é™¤äº† {deleted_count}/{len(old_sessions)} ä¸ªä¼šè¯")
    
    def merge_sessions(self, session1_id, session2_id, output_name=None):
        """åˆå¹¶ä¸¤ä¸ªä¼šè¯çš„æ•°æ®"""
        self.logger.info(f"ğŸ”„ åˆå¹¶ä¼šè¯: {session1_id} + {session2_id}")
        
        # è·å–ä¼šè¯ä¿¡æ¯
        session1_info = self._find_session(session1_id)
        session2_info = self._find_session(session2_id)
        
        if not session1_info or not session2_info:
            raise ValueError("æœªæ‰¾åˆ°æŒ‡å®šçš„ä¼šè¯")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        if not output_name:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_name = f"merged_{timestamp}"
        
        output_dir = self.output_dir / output_name
        output_dir.mkdir(exist_ok=True)
        
        # åˆå¹¶å¼•ç”¨æ ‘æ•°æ®
        merged_tree = self._merge_citation_trees(session1_info, session2_info)
        
        # ä¿å­˜åˆå¹¶åçš„æ•°æ®
        output_file = output_dir / "merged_citation_tree.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(merged_tree, f, indent=2, ensure_ascii=False)
        
        # åˆå¹¶ä¼šè¯çŠ¶æ€
        merged_state = self._merge_session_states(session1_info, session2_info)
        state_file = output_dir / "session_state.json"
        with open(state_file, 'w', encoding='utf-8') as f:
            json.dump(merged_state, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"âœ… ä¼šè¯åˆå¹¶å®Œæˆ: {output_dir}")
        return output_dir
    
    def export_session(self, session_id, format_type='txt', output_file=None):
        """å¯¼å‡ºä¼šè¯æ•°æ®ä¸ºæŒ‡å®šæ ¼å¼"""
        self.logger.info(f"ğŸ“¤ å¯¼å‡ºä¼šè¯: {session_id} (æ ¼å¼: {format_type})")
        
        session_info = self._find_session(session_id)
        if not session_info:
            raise ValueError(f"æœªæ‰¾åˆ°ä¼šè¯: {session_id}")
        
        # åŠ è½½å¼•ç”¨æ ‘æ•°æ®
        citation_files = list(session_info['path'].glob("*citation_tree*.json"))
        if not citation_files:
            raise ValueError(f"ä¼šè¯ä¸­æœªæ‰¾åˆ°å¼•ç”¨æ ‘æ•°æ®: {session_id}")
        
        with open(citation_files[0], 'r', encoding='utf-8') as f:
            tree_data = json.load(f)
        
        # ç¡®å®šè¾“å‡ºæ–‡ä»¶å
        if not output_file:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"export_{session_id}_{timestamp}.{format_type}"
        
        # æ ¹æ®æ ¼å¼å¯¼å‡º
        if format_type == 'json':
            self._export_json(tree_data, output_file)
        elif format_type == 'csv':
            self._export_csv(tree_data, output_file)
        elif format_type == 'txt':
            self._export_txt(tree_data, output_file)
        
        self.logger.info(f"âœ… å¯¼å‡ºå®Œæˆ: {output_file}")
        return output_file
    
    def _find_session(self, session_id):
        """æŸ¥æ‰¾æŒ‡å®šä¼šè¯"""
        sessions = self.get_all_sessions()
        for session in sessions:
            if session['id'] == session_id:
                return session
        return None
    
    def _merge_citation_trees(self, session1, session2):
        """åˆå¹¶ä¸¤ä¸ªä¼šè¯çš„å¼•ç”¨æ ‘æ•°æ®"""
        # åŠ è½½ä¸¤ä¸ªä¼šè¯çš„å¼•ç”¨æ ‘
        tree1_files = list(session1['path'].glob("*citation_tree*.json"))
        tree2_files = list(session2['path'].glob("*citation_tree*.json"))
        
        tree1_data = {}
        tree2_data = {}
        
        if tree1_files:
            with open(tree1_files[0], 'r', encoding='utf-8') as f:
                tree1_data = json.load(f)
        
        if tree2_files:
            with open(tree2_files[0], 'r', encoding='utf-8') as f:
                tree2_data = json.load(f)
        
        # ç®€å•åˆå¹¶ç­–ç•¥ï¼šä»¥ç¬¬ä¸€ä¸ªä¼šè¯ä¸ºåŸºç¡€ï¼Œæ·»åŠ ç¬¬äºŒä¸ªä¼šè¯çš„æ•°æ®
        merged = tree1_data.copy()
        
        if 'metadata' in tree2_data:
            merged.setdefault('metadata', {})
            merged['metadata']['merged_from'] = [session1['id'], session2['id']]
            merged['metadata']['merge_time'] = datetime.now().isoformat()
        
        # åˆå¹¶èŠ‚ç‚¹æ•°æ®ï¼ˆå¯ä»¥æ ¹æ®éœ€è¦å®ç°æ›´å¤æ‚çš„åˆå¹¶é€»è¾‘ï¼‰
        if 'root' in tree2_data and tree2_data['root']:
            if 'additional_roots' not in merged:
                merged['additional_roots'] = []
            merged['additional_roots'].append(tree2_data['root'])
        
        return merged
    
    def _merge_session_states(self, session1, session2):
        """åˆå¹¶ä¸¤ä¸ªä¼šè¯çš„çŠ¶æ€æ•°æ®"""
        state1_file = session1['path'] / "session_state.json"
        state2_file = session2['path'] / "session_state.json"
        
        state1 = {}
        state2 = {}
        
        if state1_file.exists():
            with open(state1_file, 'r', encoding='utf-8') as f:
                state1 = json.load(f)
        
        if state2_file.exists():
            with open(state2_file, 'r', encoding='utf-8') as f:
                state2 = json.load(f)
        
        # åˆå¹¶ä¼šè¯çŠ¶æ€
        merged_state = {
            'merged_from': [session1['id'], session2['id']],
            'merge_time': datetime.now().isoformat(),
            'session1_state': state1,
            'session2_state': state2,
            'request_count': state1.get('request_count', 0) + state2.get('request_count', 0),
            'visited_urls': list(set(
                state1.get('visited_urls', []) + state2.get('visited_urls', [])
            ))
        }
        
        return merged_state
    
    def _export_json(self, data, output_file):
        """å¯¼å‡ºä¸ºJSONæ ¼å¼"""
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
    
    def _export_csv(self, data, output_file):
        """å¯¼å‡ºä¸ºCSVæ ¼å¼"""
        import csv
        
        # æå–è®ºæ–‡æ•°æ®ä¸ºå¹³é¢ç»“æ„
        papers = []
        self._extract_papers_for_csv(data.get('root', {}), papers, depth=0)
        
        with open(output_file, 'w', newline='', encoding='utf-8') as f:
            if papers:
                writer = csv.DictWriter(f, fieldnames=papers[0].keys())
                writer.writeheader()
                writer.writerows(papers)
    
    def _export_txt(self, data, output_file):
        """å¯¼å‡ºä¸ºæ–‡æœ¬æ ¼å¼"""
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("PaperTracer å¼•ç”¨æ ‘å¯¼å‡º\n")
            f.write("=" * 50 + "\n\n")
            
            if 'metadata' in data:
                f.write("å…ƒæ•°æ®:\n")
                for key, value in data['metadata'].items():
                    f.write(f"  {key}: {value}\n")
                f.write("\n")
            
            if 'root' in data:
                f.write("å¼•ç”¨æ ‘ç»“æ„:\n")
                self._write_tree_structure(data['root'], f, depth=0)
    
    def _extract_papers_for_csv(self, node, papers, depth=0):
        """é€’å½’æå–è®ºæ–‡æ•°æ®ä¸ºCSVæ ¼å¼"""
        if 'paper' in node:
            paper = node['paper'].copy()
            paper['depth'] = depth
            paper['children_count'] = len(node.get('children', []))
            papers.append(paper)
        
        for child in node.get('children', []):
            self._extract_papers_for_csv(child, papers, depth + 1)
    
    def _write_tree_structure(self, node, file, depth=0):
        """é€’å½’å†™å…¥æ ‘ç»“æ„åˆ°æ–‡æœ¬æ–‡ä»¶"""
        indent = "  " * depth
        if 'paper' in node:
            paper = node['paper']
            file.write(f"{indent}- {paper.get('title', 'Unknown Title')}\n")
            file.write(f"{indent}  ä½œè€…: {paper.get('authors', 'Unknown')}\n")
            file.write(f"{indent}  å¹´ä»½: {paper.get('year', 'Unknown')}\n")
            file.write(f"{indent}  å¼•ç”¨æ¬¡æ•°: {paper.get('citation_count', 0)}\n")
            if paper.get('url'):
                file.write(f"{indent}  é“¾æ¥: {paper['url']}\n")
            file.write("\n")
        
        for child in node.get('children', []):
            self._write_tree_structure(child, file, depth + 1)

def main():
    """ä¸»å‡½æ•°"""
    parser = setup_session_manager_parser()
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return
    
    session_manager = SessionManager()
    
    try:
        if args.command == 'list':
            session_manager.list_sessions(args.recent, args.detailed)
            
        elif args.command == 'analyze':
            session_manager.analyze_session(args.session_id, args.export_stats)
            
        elif args.command == 'cleanup':
            session_manager.cleanup_sessions(args.days, args.dry_run, args.force)
            
        elif args.command == 'merge':
            session_manager.merge_sessions(args.session1, args.session2, args.output)
            
        elif args.command == 'export':
            session_manager.export_session(args.session_id, args.format, args.output)
            
    except KeyboardInterrupt:
        print("\næ“ä½œè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"é”™è¯¯: {e}")

if __name__ == "__main__":
    main()
